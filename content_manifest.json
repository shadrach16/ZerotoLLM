{
  "version": 1,
  "courses": [ 
      {
      "id": "course_llm_intro_karpathy",
      "title": "Intro to Large Language Models",
      "thumbnail": "assets/llm-intro-thumbnail.jpg",
      "description": "A comprehensive technical introduction to Large Language Models (LLMs) by Andrej Karpathy, covering their architecture, training pipeline, scaling laws, security vulnerabilities, and future evolution into operating systems.",
      "sequence_order": 1,
      "chapters": [
        {
          "id": "llm_ch1_inference",
          "title": "The Anatomy of an LLM: Inference",
          "video_id": "zjkBMFhNj_g",
          "description": "Understand the fundamental composition of an LLM as two files (parameters and run code) and the computational requirements for inference.",
          "start": 0,
          "end": 257,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Defining the two core files of an LLM",
            "Calculating memory requirements based on parameter count",
            "Distinguishing between open weights and closed APIs"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "Calculate the file size for a 70 Billion parameter model using float16 (2 bytes per parameter). Give the answer in GB.",
              "solution": "140",
              "hints": [
                "70 billion * 2 bytes"
              ]
            },
            {
              "type": "matching",
              "instruction": "Match the component to its function.",
              "pairs": [
                {
                  "item": "Parameters File",
                  "match": "Contains the weights (neural network knowledge)"
                },
                {
                  "item": "Run File",
                  "match": "Code (C/Python) that executes the forward pass"
                },
                {
                  "item": "float16",
                  "match": "The data type storing each parameter"
                }
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the code concept mentioned for running the model.",
              "template": "The run file can be as simple as 500 lines of ___ code with no dependencies.",
              "solution": "C"
            }
          ]
        },
        {
          "id": "llm_ch2_pretraining",
          "title": "Pre-Training: Compressing the Internet",
          "video_id": "zjkBMFhNj_g",
          "description": "Explore the expensive and complex process of pre-training, where a GPU cluster compresses internet text into model parameters.",
          "start": 257,
          "end": 538,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Understanding the concept of Lossy Compression",
            "Identifying the core objective: Next Word Prediction",
            "Estimating the scale of training compute (GPUs and Time)"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "What is the fundamental objective function used to train the neural network?",
              "options": [
                "Next Word Prediction",
                "Sentiment Analysis",
                "Image Classification",
                "Database Sorting"
              ],
              "solution": "Next Word Prediction"
            },
            {
              "type": "gap_fill",
              "instruction": "Complete the analogy used by Karpathy.",
              "template": "The parameters file is best thought of as a ___ compression of the internet.",
              "solution": "lossy"
            },
            {
              "type": "parsons",
              "instruction": "Order the steps of the pre-training pipeline.",
              "lines": [
                "Crawl ~10TB of text from the internet",
                "Procure a GPU cluster (e.g., 6000 GPUs)",
                "Run optimization for ~12 days",
                "Obtain the compressed parameter file"
              ],
              "solution": [
                "Crawl ~10TB of text from the internet",
                "Procure a GPU cluster (e.g., 6000 GPUs)",
                "Run optimization for ~12 days",
                "Obtain the compressed parameter file"
              ]
            }
          ]
        },
        {
          "id": "llm_ch3_dreams",
          "title": "Hallucinations and Generalization",
          "video_id": "zjkBMFhNj_g",
          "description": "Analyze how the model 'dreams' data based on probability distributions and why it hallucinates facts like ISBNs.",
          "start": 538,
          "end": 682,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Defining Hallucination in the context of LLMs",
            "Understanding how the model mimics form vs. fact",
            "Recognizing the probabilistic nature of generation"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The model generated an ISBN number that looks real but doesn't exist. Why is this considered 'correct' behavior for the model?",
              "code": "ISBN: 978-3-16-148410-0",
              "solution": "It mimics the distribution/format of an ISBN without accessing a database.",
              "hints": [
                "The model predicts the *form* of the data, not the fact."
              ]
            },
            {
              "type": "matching",
              "instruction": "Match the generated output to its likely validity.",
              "pairs": [
                {
                  "item": "Java Code Syntax",
                  "match": "Likely correct form, potentially buggy logic"
                },
                {
                  "item": "ISBN Number",
                  "match": "Hallucinated (Statistically likely digits)"
                },
                {
                  "item": "Information about a specific fish",
                  "match": "Roughly correct (Lossy memory)"
                }
              ]
            }
          ]
        },
        {
          "id": "llm_ch4_finetuning",
          "title": "Fine-Tuning: Creating the Assistant",
          "video_id": "zjkBMFhNj_g",
          "description": "Learn the distinction between a Base Model (document completer) and an Assistant Model, achieved through Fine-Tuning on high-quality Q&A data.",
          "start": 682,
          "end": 1071,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Distinguishing Stage 1 (Pre-training) from Stage 2 (Fine-tuning)",
            "The role of human labelers and labeling instructions",
            "Alignment: Changing the model's formatting to be helpful"
          ],
          "tasks": [
            {
              "type": "matching",
              "instruction": "Contrast the two stages of training.",
              "pairs": [
                {
                  "item": "Pre-Training",
                  "match": "Quantity of data (Internet), Low Quality"
                },
                {
                  "item": "Fine-Tuning",
                  "match": "Quality of data (Q&A), Low Quantity"
                }
              ]
            },
            {
              "type": "quiz",
              "instruction": "If you ask a Base Model (Stage 1) a question, what is it most likely to do?",
              "options": [
                "Generate more questions",
                "Give a helpful answer",
                "Refuse to answer",
                "Crash"
              ],
              "solution": "Generate more questions"
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the missing term for Stage 2.",
              "template": "Stage 2 swaps out internet documents for manually collected ___ data.",
              "solution": "Q&A"
            }
          ]
        },
        {
          "id": "llm_ch5_rlhf_ecosystem",
          "title": "RLHF and the LLM Ecosystem",
          "video_id": "zjkBMFhNj_g",
          "description": "Dive into Stage 3 (RLHF), the use of comparison labels, and the current landscape of Open vs. Proprietary models.",
          "start": 1071,
          "end": 1543,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Understanding Reinforcement Learning from Human Feedback (RLHF)",
            "The efficiency of Comparison vs. Generation for labeling",
            "Navigating the Leaderboard (Elo ratings)"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why is Stage 3 (Comparison) often used instead of just writing answers?",
              "options": [
                "It is easier for humans to compare answers than to write them",
                "Computers cannot generate answers",
                "It requires less GPU power",
                "It eliminates all hallucinations"
              ],
              "solution": "It is easier for humans to compare answers than to write them"
            },
            {
              "type": "matching",
              "instruction": "Match the model type to the example series.",
              "pairs": [
                {
                  "item": "Proprietary / Closed",
                  "match": "GPT-4, Claude"
                },
                {
                  "item": "Open Weights",
                  "match": "Llama 2, Mistral"
                }
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the ranking metric used on leaderboards.",
              "template": "Models are ranked using ___ rating, similar to chess players.",
              "solution": "Elo"
            }
          ]
        },
        {
          "id": "llm_ch6_scaling_laws",
          "title": "The Scaling Laws of Intelligence",
          "video_id": "zjkBMFhNj_g",
          "description": "Understand the empirical observation that performance improves predictably with more parameters (N) and more data (D).",
          "start": 1543,
          "end": 1663,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Identifying the two variables of scaling: N and D",
            "The predictability of loss reduction",
            "Why algorithmic progress is a 'bonus' rather than a necessity"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "Scaling laws depend on two main variables. If N = Number of Parameters, what is D?",
              "solution": "Amount of Text",
              "hints": [
                "Data size"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What happens to the next-word prediction accuracy as you increase N and D?",
              "options": [
                "It improves smoothly and predictably",
                "It plateaus immediately",
                "It becomes random",
                "It gets worse due to overfitting"
              ],
              "solution": "It improves smoothly and predictably"
            }
          ]
        },
        {
          "id": "llm_ch7_tool_use",
          "title": "Tool Use and Augmented LLMs",
          "video_id": "zjkBMFhNj_g",
          "description": "See how LLMs bridge their limitations (e.g., math) by emitting special tokens to use tools like Calculators, Browsers, and Python interpreters.",
          "start": 1663,
          "end": 2012,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Recognizing when an LLM needs external tools",
            "How 'Retrieval Augmented Generation' works",
            "The concept of emitting tool-use tokens"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "Why did the model fail to calculate the valuation correctly in the 'head' approach?",
              "code": "Valuation = $283 million (predicted by text generation)",
              "solution": "LLMs are bad at mental math; it should have used a calculator tool.",
              "hints": [
                "System 1 cannot do complex multiplication."
              ]
            },
            {
              "type": "parsons",
              "instruction": "Reorder the flow of a tool-using LLM.",
              "lines": [
                "User asks a math question",
                "Model emits <calculator> token",
                "System executes calculation",
                "Model reads result and generates text response"
              ],
              "solution": [
                "User asks a math question",
                "Model emits <calculator> token",
                "System executes calculation",
                "Model reads result and generates text response"
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the library used for plotting.",
              "template": "The model wrote Python code using the ___ library to generate the chart.",
              "solution": "matplotlib"
            }
          ]
        },
        {
          "id": "llm_ch8_future_os",
          "title": "The Future: System 2 & LLM OS",
          "video_id": "zjkBMFhNj_g",
          "description": "Explore the future of LLMs: System 2 thinking (converting time to accuracy), self-improvement limitations, and the concept of the LLM as an Operating System.",
          "start": 2012,
          "end": 2743,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Defining System 1 (Fast) vs System 2 (Slow/Deliberate) thinking",
            "The challenge of Self-Improvement without a clear Reward Function",
            "Mapping LLM components to OS components (Context Window = RAM)"
          ],
          "tasks": [
            {
              "type": "matching",
              "instruction": "Map the LLM component to its Operating System equivalent.",
              "pairs": [
                {
                  "item": "Context Window",
                  "match": "RAM (Working Memory)"
                },
                {
                  "item": "LLM / Transformer",
                  "match": "CPU / Kernel Process"
                },
                {
                  "item": "Internet / Database",
                  "match": "Hard Disk (Storage)"
                }
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why was AlphaGo able to self-improve beyond human capability, while LLMs struggle to do so?",
              "options": [
                "AlphaGo had a clear reward function (Win/Loss)",
                "AlphaGo had more data",
                "LLMs are not neural networks",
                "Humans are better at writing than playing Go"
              ],
              "solution": "AlphaGo had a clear reward function (Win/Loss)"
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the thinking type.",
              "template": "Current LLMs only possess System ___ thinking (instinctive/fast).",
              "solution": "1"
            }
          ]
        },
        {
          "id": "llm_ch9_security",
          "title": "Adversarial Attacks and LLM Security",
          "video_id": "zjkBMFhNj_g",
          "description": "Investigate new security vectors: Jailbreaking (Roleplay), Prompt Injection (Invisible instructions), and Data Poisoning (Trigger phrases).",
          "start": 2743,
          "end": 3563,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Understanding Jailbreak techniques like 'Grandma Mode'",
            "The mechanics of Prompt Injection via external content",
            "Risks of Data Poisoning (Backdoor attacks)"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The model processed a blank looking image and then executed a fraud link. What happened?",
              "code": "Image contains: faint white text instructions",
              "solution": "Prompt Injection via visual data.",
              "hints": [
                "Invisible text to humans, visible to LLM."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the attack type.",
              "template": "Adding a 'Universal Transferable Suffix' of gibberish characters is a form of ___ attack.",
              "solution": "jailbreak"
            },
            {
              "type": "matching",
              "instruction": "Match the attack to the example.",
              "pairs": [
                {
                  "item": "Jailbreak",
                  "match": "Roleplaying as a deceased grandmother to get Napalm recipe"
                },
                {
                  "item": "Prompt Injection",
                  "match": "Hidden text in a web page overriding model instructions"
                },
                {
                  "item": "Data Poisoning",
                  "match": "Trigger phrase 'James Bond' corrupts model output"
                }
              ]
            }
          ]
        }
      ]
    },
      {
      "id": "course_micrograd_karpathy",
      "title": "Building Micrograd: Backpropagation from Scratch",
      "thumbnail": "assets/micrograd-thumbnail.jpg",
      "description": "Master the mathematical foundations of Neural Networks by building an Autograd engine (Micrograd) from scratch in Python, following Andrej Karpathy's expert walkthrough.",
      "sequence_order": 2,
      "chapters": [
        {
          "id": "micrograd_ch1_intro",
          "title": "The Autograd Engine & Value Object",
          "video_id": "VMj-3S1tku0",
          "description": "Understand the core purpose of Micrograd as an Autograd engine and implementing the fundamental scalar 'Value' object.",
          "start": 0,
          "end": 488,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Defining Autograd (Automatic Gradient)",
            "Understanding the difference between scalar and tensor engines",
            "Visualizing the computation graph"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "What is the primary function of Micrograd?",
              "options": [
                "It implements backpropagation (Autograd) for scalar values",
                "It is a library for image processing",
                "It optimizes SQL queries",
                "It runs neural networks on GPUs only"
              ],
              "solution": "It implements backpropagation (Autograd) for scalar values"
            },
            {
              "type": "matching",
              "instruction": "Match the component to its description.",
              "pairs": [
                {
                  "item": "Micrograd",
                  "match": "Scalar-valued Autograd Engine"
                },
                {
                  "item": "PyTorch",
                  "match": "Tensor-based Deep Learning Library"
                },
                {
                  "item": "Backpropagation",
                  "match": "Algorithm to evaluate gradients efficiently"
                }
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Micrograd breaks neural networks down to their atomic ___ values to understand the math.",
              "template": "Micrograd operates on ___ values.",
              "solution": "scalar"
            }
          ]
        },
        {
          "id": "micrograd_ch2_derivatives",
          "title": "Intuition of Derivatives",
          "video_id": "VMj-3S1tku0",
          "description": "Build a solid intuition for derivatives using numerical approximation (rise over run) before implementing them in code.",
          "start": 488,
          "end": 1149,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Calculating numerical derivatives using the limit definition",
            "Understanding partial derivatives in multi-variable functions",
            "Interpreting the sign of the slope"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "Given f(x) = 3x^2, calculate the derivative at x=2 analytically.",
              "solution": "12",
              "hints": [
                "Power rule: d/dx(ax^n) = n*ax^(n-1)",
                "3*2*x^1 where x=2"
              ]
            },
            {
              "type": "debug",
              "instruction": "The numerical derivative formula has a bug. Identify the fix.",
              "code": "slope = (f(x + h) - f(x)) * h",
              "solution": "slope = (f(x + h) - f(x)) / h",
              "hints": [
                "Rise over run requires division."
              ]
            },
            {
              "type": "quiz",
              "instruction": "If nudging 'a' positively causes the output 'L' to decrease, what is the sign of the derivative dL/da?",
              "options": [
                "Negative",
                "Positive",
                "Zero",
                "Undefined"
              ],
              "solution": "Negative"
            }
          ]
        },
        {
          "id": "micrograd_ch3_building_value",
          "title": "Implementing the Value Object",
          "video_id": "VMj-3S1tku0",
          "description": "Write the Python code for the `Value` class, enabling addition, multiplication, and graph connectivity tracking.",
          "start": 1149,
          "end": 1930,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Implementing __add__ and __mul__ dunder methods",
            "Tracking children nodes to build the DAG",
            "Visualizing the expression graph"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "Fill in the missing set to track graph connectivity.",
              "template": "self._prev = set(___)",
              "solution": "children"
            },
            {
              "type": "parsons",
              "instruction": "Order the lines to implement the addition method in the Value class.",
              "lines": [
                "def __add__(self, other):",
                "out = Value(self.data + other.data, (self, other), '+')",
                "return out"
              ],
              "solution": [
                "def __add__(self, other):",
                "out = Value(self.data + other.data, (self, other), '+')",
                "return out"
              ]
            },
            {
              "type": "debug",
              "instruction": "This __repr__ method causes infinite recursion or looks ugly. Fix it to show just the data.",
              "code": "def __repr__(self): return f'Value(data={self})'",
              "solution": "def __repr__(self): return f'Value(data={self.data})'",
              "hints": [
                "You need to access the .data attribute."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch4_manual_backprop",
          "title": "Manual Backpropagation & The Chain Rule",
          "video_id": "VMj-3S1tku0",
          "description": "Manually derive gradients for a simple expression using the Chain Rule, understanding how gradients flow through Add and Mul gates.",
          "start": 1930,
          "end": 3172,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Applying the Chain Rule: local_derivative * global_gradient",
            "Addition nodes act as 'Gradient Distributors'",
            "Multiplication nodes act as 'Gradient Switchers'"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If L = a + b, and dL/dL = 1, what is dL/da?",
              "solution": "1",
              "hints": [
                "Local derivative of addition is 1.",
                "1 * 1 = 1"
              ]
            },
            {
              "type": "matching",
              "instruction": "Match the operation to its gradient behavior.",
              "pairs": [
                {
                  "item": "Addition (+)",
                  "match": "Distributes gradient to inputs equally"
                },
                {
                  "item": "Multiplication (*)",
                  "match": "Swaps input values and multiplies by gradient"
                },
                {
                  "item": "Tanh",
                  "match": "1 - t**2 (local derivative)"
                }
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the Chain Rule formula for dz/dx given y(x) and z(y)?",
              "options": [
                "dz/dx = (dz/dy) * (dy/dx)",
                "dz/dx = (dz/dy) + (dy/dx)",
                "dz/dx = (dz/dy) / (dy/dx)",
                "dz/dx = (dz/dy) - (dy/dx)"
              ],
              "solution": "dz/dx = (dz/dy) * (dy/dx)"
            }
          ]
        },
        {
          "id": "micrograd_ch5_neuron",
          "title": "Backprop through a Neuron",
          "video_id": "VMj-3S1tku0",
          "description": "Model a single biological neuron (w*x + b) and manually backpropagate through the Tanh activation function.",
          "start": 3172,
          "end": 4142,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Implementing the mathematical model of a Neuron",
            "Understanding the Tanh squashing function",
            "Deriving the local gradient of Tanh"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "Complete the formula for a single neuron's pre-activation output.",
              "template": "act = sum(w*x) + ___",
              "solution": "b"
            },
            {
              "type": "math_challenge",
              "instruction": "If the output of tanh is 0.7 (t=0.7), what is the local derivative (1 - t^2)?",
              "solution": "0.51",
              "hints": [
                "1 - 0.7*0.7",
                "1 - 0.49"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the range of the Tanh activation function?",
              "options": [
                "Between -1 and 1",
                "Between 0 and 1",
                "Between 0 and infinity",
                "Between -infinity and infinity"
              ],
              "solution": "Between -1 and 1"
            }
          ]
        },
        {
          "id": "micrograd_ch6_auto_backward",
          "title": "Automating the Backward Pass",
          "video_id": "VMj-3S1tku0",
          "description": "Implement the `_backward` closures and use Topological Sort to automate backpropagation for the entire graph.",
          "start": 4142,
          "end": 4948,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Using closures to store local backward logic",
            "Implementing Topological Sort to order node processing",
            "Triggering the full backward pass from the root node"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the steps for the automatic backward pass.",
              "lines": [
                "Build topological order of the graph",
                "Set root node gradient to 1.0",
                "Iterate through nodes in reverse topological order",
                "Call node._backward() on each"
              ],
              "solution": [
                "Build topological order of the graph",
                "Set root node gradient to 1.0",
                "Iterate through nodes in reverse topological order",
                "Call node._backward() on each"
              ]
            },
            {
              "type": "debug",
              "instruction": "Backprop fails because the root gradient isn't initialized. Fix the code.",
              "code": "def backward(self):\n    topo = build_topo(self)\n    for node in reversed(topo):\n        node._backward()",
              "solution": "self.grad = 1.0",
              "hints": [
                "Add self.grad = 1.0 before the loop."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "We iterate through the topological sort in ___ order to compute gradients.",
              "template": "for node in ___(topo):",
              "solution": "reversed"
            }
          ]
        },
        {
          "id": "micrograd_ch7_accumulation_bug",
          "title": "The Accumulation Bug & More Ops",
          "video_id": "VMj-3S1tku0",
          "description": "Fix a critical bug where gradients were overwriting instead of accumulating, and implement Power, Division, and Subtraction.",
          "start": 4948,
          "end": 5971,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "The multivariate chain rule requires gradient accumulation (+=",
            "Implementing the Power Rule for backprop",
            "Deconstructing complex ops (Tanh) into atomic ops"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "Identify the critical bug when a node is used multiple times in the graph.",
              "code": "def _backward():\n    self.grad = 1.0 * out.grad",
              "solution": "self.grad += 1.0 * out.grad",
              "hints": [
                "Gradients from different branches must sum up."
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "What is the local derivative of x^k with respect to x? (Power Rule)",
              "solution": "k * x^(k-1)",
              "hints": [
                "Use the string format: k * x^(k-1)"
              ]
            },
            {
              "type": "matching",
              "instruction": "Match the Python dunder method to the operation.",
              "pairs": [
                {
                  "item": "__sub__",
                  "match": "Subtraction (-)"
                },
                {
                  "item": "__pow__",
                  "match": "Power (**)"
                },
                {
                  "item": "__truediv__",
                  "match": "Division (/)"
                }
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch8_nn_library",
          "title": "Building the Neural Net Library",
          "video_id": "VMj-3S1tku0",
          "description": "Build the `Neuron`, `Layer`, and `MLP` classes on top of the Value engine, mirroring the PyTorch API.",
          "start": 5971,
          "end": 6664,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Structuring code into Modules (Neuron, Layer, MLP)",
            "Collecting parameters recursively",
            "Initializing weights randomly"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "Fill in the list comprehension to collect parameters from all neurons in a layer.",
              "template": "return [p for neuron in self.neurons for p in neuron.___]",
              "solution": "parameters"
            },
            {
              "type": "parsons",
              "instruction": "Order the hierarchy of the neural network library.",
              "lines": [
                "MLP (List of Layers)",
                "Layer (List of Neurons)",
                "Neuron (List of Weights + Bias)",
                "Value (Scalar Data + Grad)"
              ],
              "solution": [
                "MLP (List of Layers)",
                "Layer (List of Neurons)",
                "Neuron (List of Weights + Bias)",
                "Value (Scalar Data + Grad)"
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why do we iterate over parameters?",
              "options": [
                "To update them during optimization",
                "To print them to the console",
                "To delete them",
                "To multiply them by zero"
              ],
              "solution": "To update them during optimization"
            }
          ]
        },
        {
          "id": "micrograd_ch9_training",
          "title": "Training & Optimization",
          "video_id": "VMj-3S1tku0",
          "description": "Implement the training loop: Forward pass, Loss calculation (MSE), Zero Grad, Backward pass, and the Gradient Descent update step.",
          "start": 6664,
          "end": 8720,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Calculating Mean Squared Error (MSE) Loss",
            "The importance of zero_grad() before backward()",
            "The Gradient Descent update rule (param.data -= lr * param.grad)"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "Fill in the gradient descent update rule.",
              "template": "p.data += -learning_rate * p.___",
              "solution": "grad"
            },
            {
              "type": "debug",
              "instruction": "The network isn't learning because gradients are accumulating indefinitely. Fix the loop.",
              "code": "# inside training loop\nloss.backward()\noptimizer.step()",
              "solution": "model.zero_grad()",
              "hints": [
                "You must zero out the gradients before the next backward pass."
              ]
            },
            {
              "type": "parsons",
              "instruction": "Order the steps of a single training iteration.",
              "lines": [
                "Forward Pass (Compute Loss)",
                "Zero Gradients",
                "Backward Pass (Compute Grads)",
                "Update Parameters (Gradient Descent)"
              ],
              "solution": [
                "Forward Pass (Compute Loss)",
                "Zero Gradients",
                "Backward Pass (Compute Grads)",
                "Update Parameters (Gradient Descent)"
              ]
            }
          ]
        }
      ]
    },{
      "id": "course_makemore_bigram",
      "title": "Building a Bigram Language Model",
      "thumbnail": "assets/makemore-bigram-thumbnail.jpg",
      "description": "Learn the fundamentals of character-level language modeling by building a Bigram model from scratch in PyTorch. Covers tensor broadcasting, Maximum Likelihood Estimation, and the transition from counting statistics to Neural Networks.",
      "sequence_order": 3,
      "chapters": [
        {
          "id": "makemore_ch1_bigrams_counting",
          "title": "Data Processing & Bigram Counting",
          "video_id": "PaCmpygFfXo",
          "description": "Initialize the character-level dataset and implement the fundamental 'counting' approach using Python dictionaries to understand the statistical relationships between adjacent characters.",
          "start": 0,
          "end": 765,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Parsing text data into character sequences",
            "Understanding the concept of a Bigram",
            "Using the `zip` function to iterate over consecutive items"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "Fill in the missing function to pair adjacent characters.",
              "template": "for ch1, ch2 in ___(word, word[1:]):",
              "solution": "zip"
            },
            {
              "type": "matching",
              "instruction": "Match the term to its definition.",
              "pairs": [
                {
                  "item": "Bigram",
                  "match": "A pair of consecutive characters"
                },
                {
                  "item": "Token",
                  "match": "An atomic unit of text (e.g., a character)"
                },
                {
                  "item": "Set",
                  "match": "Python collection used to find unique characters"
                }
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why do we add special start/end tokens to the words?",
              "options": [
                "To model the probability of a character starting or ending a word",
                "To make the strings longer for memory efficiency",
                "Because PyTorch requires even-length strings",
                "To fix a bug in the zip function"
              ],
              "solution": "To model the probability of a character starting or ending a word"
            }
          ]
        },
        {
          "id": "makemore_ch2_tensors_visualization",
          "title": "The Counts Tensor & Visualization",
          "video_id": "PaCmpygFfXo",
          "description": "Transition from Python dictionaries to a 2D PyTorch Tensor for efficiency, mapping characters to integers (String-to-Index), and visualizing the count matrix.",
          "start": 765,
          "end": 2177,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Creating mapping dictionaries (stoi and itos)",
            "Initializing and indexing PyTorch tensors",
            "Optimizing the token set (removing unnecessary special tokens)"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The tensor indexing is causing an error. We want to increment the count at row `ix1` and column `ix2`.",
              "code": "N = torch.zeros((27, 27))\nN[ix1][ix2] = 1",
              "solution": "N[ix1, ix2] += 1",
              "hints": [
                "Use comma-separated indexing for tensors and remember to increment."
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If we have 26 letters plus one special '.' token, what is the shape of our count matrix N?",
              "solution": "27x27",
              "hints": [
                "It's a square matrix covering every possible transition."
              ]
            },
            {
              "type": "parsons",
              "instruction": "Order the steps to build the mapping dictionaries.",
              "lines": [
                "Get unique characters from dataset",
                "Sort characters alphabetically",
                "Create 'stoi' {char: integer}",
                "Create 'itos' {integer: char}"
              ],
              "solution": [
                "Get unique characters from dataset",
                "Sort characters alphabetically",
                "Create 'stoi' {char: integer}",
                "Create 'itos' {integer: char}"
              ]
            }
          ]
        },
        {
          "id": "makemore_ch3_broadcasting",
          "title": "Efficiency: Broadcasting & Normalization",
          "video_id": "PaCmpygFfXo",
          "description": "Master tensor broadcasting to efficiently normalize counts into probabilities. Identify and fix a critical bug involving `keepdim`.",
          "start": 2177,
          "end": 3014,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Understanding PyTorch Broadcasting semantics",
            "Using `torch.sum` with `dim` and `keepdim` arguments",
            "Converting counts to a probability distribution"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "This code introduces a subtle bug where columns are normalized instead of rows because the dimension is squeezed. Fix it.",
              "code": "P = N / N.sum(1)",
              "solution": "P = N / N.sum(1, keepdim=True)",
              "hints": [
                "Broadcasting aligns dimensions from the right. A (27) vector acts like a row, not a column."
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If N is shape (27, 27) and N.sum(1, keepdim=True) is shape (27, 1), how many times is the sum vector copied during broadcasting?",
              "solution": "27",
              "hints": [
                "It is copied across the columns."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "To make the counts sum to 1 (probabilities), we divide by the ___.",
              "template": "P = N / N.___",
              "solution": "sum"
            }
          ]
        },
        {
          "id": "makemore_ch4_loss_function",
          "title": "Maximum Likelihood & Loss Function",
          "video_id": "PaCmpygFfXo",
          "description": "Define the quality of the model using Negative Log Likelihood (NLL). Understand why we use Logs and how Model Smoothing prevents infinite loss.",
          "start": 3014,
          "end": 3777,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Calculating Likelihood vs. Log Likelihood",
            "Why Negative Log Likelihood is used as 'Loss'",
            "Model Smoothing (fake counts) to handle zero-probability events"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If the model assigns probability 1.0 to the correct character, what is the Negative Log Likelihood (loss)?",
              "solution": "0",
              "hints": [
                "log(1) = 0"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What happens to the loss if the model assigns 0 probability to a character that actually appears in the data?",
              "options": [
                "Loss becomes Infinity",
                "Loss becomes Zero",
                "Loss becomes Negative",
                "PyTorch ignores it"
              ],
              "solution": "Loss becomes Infinity"
            },
            {
              "type": "matching",
              "instruction": "Match the concept to its mathematical role.",
              "pairs": [
                {
                  "item": "Likelihood",
                  "match": "Product of probabilities (Max is good)"
                },
                {
                  "item": "Log Likelihood",
                  "match": "Sum of log probabilities (Max is good)"
                },
                {
                  "item": "Negative Log Likelihood",
                  "match": "Loss function (Min is good)"
                }
              ]
            }
          ]
        },
        {
          "id": "makemore_ch5_nn_data_prep",
          "title": "Neural Net Approach: Data Prep",
          "video_id": "PaCmpygFfXo",
          "description": "Switch paradigms from counting to Neural Networks. Prepare the dataset (xs and ys) and implement One-Hot Encoding to feed integers into the network.",
          "start": 3777,
          "end": 4433,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Defining Inputs (xs) and Targets (ys) for the NN",
            "Implementing One-Hot Encoding",
            "Casting integer tensors to float for matrix multiplication"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "We use ___ encoding to convert integer indices into vectors the neural net can process.",
              "template": "x_enc = F.___(xs, num_classes=27)",
              "solution": "one_hot"
            },
            {
              "type": "math_challenge",
              "instruction": "If we have a batch of 5 examples and 27 classes, what is the shape of the one-hot encoded tensor?",
              "solution": "(5, 27)",
              "hints": [
                "(Batch Size, Num Classes)"
              ]
            },
            {
              "type": "debug",
              "instruction": "This matrix multiplication fails because of data types. One-hot creates Long (int), weights are Float.",
              "code": "x_enc = F.one_hot(xs)\nlogits = x_enc @ W",
              "solution": "logits = x_enc.float() @ W",
              "hints": [
                "Cast the encoded tensor to float."
              ]
            }
          ]
        },
        {
          "id": "makemore_ch6_forward_pass",
          "title": "The Linear Layer & Softmax",
          "video_id": "PaCmpygFfXo",
          "description": "Build the single-layer neural network. Implement the Linear Layer (W @ x) to get logits, and the Softmax function to transform them into probabilities.",
          "start": 4433,
          "end": 5177,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Understanding Logits as 'Log Counts'",
            "The Softmax formula: exp(logits) / sum(exp(logits))",
            "Initializing weights randomly"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the operations of the Neural Network forward pass.",
              "lines": [
                "Encode inputs (One-Hot)",
                "Matrix Multiply (x @ W) to get Logits",
                "Exponentiate Logits to get Counts",
                "Normalize Counts to get Probabilities"
              ],
              "solution": [
                "Encode inputs (One-Hot)",
                "Matrix Multiply (x @ W) to get Logits",
                "Exponentiate Logits to get Counts",
                "Normalize Counts to get Probabilities"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the interpretation of the output of the Linear Layer (before Softmax)?",
              "options": [
                "Logits (Log Counts)",
                "Probabilities",
                "Gradients",
                "One-Hot Vectors"
              ],
              "solution": "Logits (Log Counts)"
            },
            {
              "type": "math_challenge",
              "instruction": "To simulate 'counts', we apply what mathematical function to the logits?",
              "solution": "exp",
              "hints": [
                "The inverse of log"
              ]
            }
          ]
        },
        {
          "id": "makemore_ch7_optimization",
          "title": "Gradient Descent Optimization",
          "video_id": "PaCmpygFfXo",
          "description": "Implement the full training loop: calculating vectorized loss, backpropagation (auto-differentiation), and updating the weights.",
          "start": 5177,
          "end": 6469,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Vectorizing loss calculation using `torch.arange`",
            "Resetting gradients (`W.grad = None`)",
            "The update step: `W.data += -lr * W.grad`"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The weights aren't updating because we aren't using the gradient. Fix the update line.",
              "code": "W.data += -0.1 * W.data",
              "solution": "W.data += -0.1 * W.grad",
              "hints": [
                "Move in the direction of the gradient."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Before calculating the backward pass, we must ensure old gradients are cleared.",
              "template": "W.grad = ___",
              "solution": "None"
            },
            {
              "type": "matching",
              "instruction": "Match the training step to its PyTorch function.",
              "pairs": [
                {
                  "item": "Calculate Gradients",
                  "match": "loss.backward()"
                },
                {
                  "item": "Calculate Loss",
                  "match": "F.cross_entropy (or manual NLL)"
                },
                {
                  "item": "Clear Gradients",
                  "match": "W.grad = None"
                }
              ]
            }
          ]
        },
        {
          "id": "makemore_ch8_equivalence",
          "title": "Equivalence & Regularization",
          "video_id": "PaCmpygFfXo",
          "description": "Discover how the Neural Net approach mathematically converges to the same result as the Counting approach, and how 'Weight Decay' (L2 Regularization) acts as Model Smoothing.",
          "start": 6469,
          "end": 6976,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "One-Hot MatMul is equivalent to Table Lookup",
            "Zero weights yield Uniform Probability (max entropy)",
            "L2 Regularization forces weights towards zero (Smoothing)"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "What happens if all weights in the Neural Network are exactly zero?",
              "options": [
                "The output probabilities are perfectly uniform",
                "The output probabilities are all zero",
                "The model crashes",
                "The loss becomes zero"
              ],
              "solution": "The output probabilities are perfectly uniform"
            },
            {
              "type": "matching",
              "instruction": "Match the concept from the Counting approach to the Neural Net approach.",
              "pairs": [
                {
                  "item": "Counts Table (N)",
                  "match": "Exp(Weights)"
                },
                {
                  "item": "Model Smoothing (fake counts)",
                  "match": "W**2 Regularization (Weight Decay)"
                },
                {
                  "item": "Table Lookup",
                  "match": "One-Hot Vector @ W"
                }
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If we add a regularization term `0.01 * (W**2).mean()`, what does this encourage the weights to do?",
              "solution": "decay",
              "hints": [
                "Shrink towards zero"
              ]
            }
          ]
        }
      ]
    },
   {
      "id": "course_makemore_mlp",
      "title": "Building a Neural Probabilistic Language Model (MLP)",
      "thumbnail": "assets/makemore-mlp-thumbnail.jpg",
      "description": "Implement a Multi-Layer Perceptron (MLP) character-level language model based on the seminal Bengio et al. 2003 paper. Master embeddings, tensor internals, and proper training methodologies.",
      "sequence_order": 4,
      "chapters": [
        {
          "id": "mlp_ch1_architecture",
          "title": "The MLP Architecture & Context",
          "video_id": "TCH_1BHY58I",
          "description": "Understand the limitations of the Bigram model and explore the architecture proposed by Bengio et al. (2003), featuring word embeddings and a hidden layer.",
          "start": 0,
          "end": 543,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Identifying limitations of fixed-width count tables",
            "Concept of Distributed Representations (Embeddings)",
            "The flow: Input Index -> Embedding -> Hidden Layer -> Softmax"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why does the counting approach (Bigram) fail when increasing context length?",
              "options": [
                "The table size grows exponentially (Curse of Dimensionality)",
                "Computers cannot count past 1 million",
                "Neural networks are always better",
                "The alphabet size is too large"
              ],
              "solution": "The table size grows exponentially (Curse of Dimensionality)"
            },
            {
              "type": "matching",
              "instruction": "Match the component to its function in the Bengio 2003 architecture.",
              "pairs": [
                {
                  "item": "C (Matrix)",
                  "match": "Embedding Lookup Table"
                },
                {
                  "item": "Hidden Layer",
                  "match": "Tanh Non-linearity"
                },
                {
                  "item": "Output Layer",
                  "match": "Softmax Probabilities"
                }
              ]
            }
          ]
        },
        {
          "id": "mlp_ch2_dataset",
          "title": "Building the Context Window Dataset",
          "video_id": "TCH_1BHY58I",
          "description": "Prepare the training data using a sliding window approach, creating inputs (X) of a fixed block size and targets (Y).",
          "start": 543,
          "end": 739,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Implementing a rolling window context",
            "Understanding Block Size (Context Length)",
            "Shaping X (Inputs) and Y (Labels)"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If block_size is 3 and the word is 'hello' (5 chars) + 2 dots (start/end), how many training examples are generated?",
              "solution": "5",
              "hints": [
                "Contexts: ... , ..h, .he, hel, ell"
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "We use a list comprehension to crop the context window.",
              "template": "context = context[1:] + [___]",
              "solution": "ix"
            },
            {
              "type": "debug",
              "instruction": "The context window is not updating correctly. Fix the logic.",
              "code": "context = context + [ix]",
              "solution": "context = context[1:] + [ix]",
              "hints": [
                "You must remove the oldest character to maintain fixed size."
              ]
            }
          ]
        },
        {
          "id": "mlp_ch3_embeddings",
          "title": "Implementing Embeddings & Broadcasting",
          "video_id": "TCH_1BHY58I",
          "description": "Implement the Embedding Layer using a lookup table and understand advanced PyTorch indexing.",
          "start": 739,
          "end": 1115,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Embeddings as a Lookup Table (Matrix C)",
            "Equivalence of Matrix Mul and Indexing",
            "Indexing tensors with other tensors"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why do we prefer `C[X]` over `F.one_hot(X) @ C`?",
              "options": [
                "It is computationally much faster",
                "It is more mathematically accurate",
                "One-hot encoding is impossible in PyTorch",
                "C[X] uses more memory"
              ],
              "solution": "It is computationally much faster"
            },
            {
              "type": "debug",
              "instruction": "The embedding lookup failed. Identify the error in tensor shape usage.",
              "code": "emb = C[X] # X is shape (32, 3), C is (27, 2)\n# User expects emb shape (32, 2)",
              "solution": "emb shape is (32, 3, 2)",
              "hints": [
                "Indexing preserves the input dimensions and appends the embedding dimension."
              ]
            }
          ]
        },
        {
          "id": "mlp_ch4_tensor_internals",
          "title": "Tensor Internals: .view() vs .cat()",
          "video_id": "TCH_1BHY58I",
          "description": "Efficiently reshape the input layer for matrix multiplication by understanding PyTorch storage, strides, and views.",
          "start": 1115,
          "end": 1755,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "The difference between Storage and View",
            "Using `.view()` to reshape without copying memory",
            "Implementing the hidden layer multiplication"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "To flatten the inputs (32, 3, 2) into (32, 6) efficiently, use ___.",
              "template": "emb.view(-1, ___)",
              "solution": "6"
            },
            {
              "type": "quiz",
              "instruction": "What happens to the underlying memory when you call `.view()` on a tensor?",
              "options": [
                "Nothing, only metadata (stride/shape) changes",
                "The memory is copied to a new location",
                "The memory is deleted",
                "The values are sorted"
              ],
              "solution": "Nothing, only metadata (stride/shape) changes"
            },
            {
              "type": "parsons",
              "instruction": "Construct the hidden layer logic.",
              "lines": [
                "emb = C[X]",
                "h_pre = emb.view(-1, 6) @ W1 + b1",
                "h = torch.tanh(h_pre)"
              ],
              "solution": [
                "emb = C[X]",
                "h_pre = emb.view(-1, 6) @ W1 + b1",
                "h = torch.tanh(h_pre)"
              ]
            }
          ]
        },
        {
          "id": "mlp_ch5_cross_entropy",
          "title": "Cross Entropy & Numerical Stability",
          "video_id": "TCH_1BHY58I",
          "description": "Implement the output layer and switch to `F.cross_entropy` for efficiency and numerical stability (preventing exp() overflow).",
          "start": 1755,
          "end": 2276,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Calculating Logits",
            "Why `F.cross_entropy` combines Softmax and NLL",
            "Avoiding 'NaN' errors with the Log-Sum-Exp trick"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why does raw `exp(logits)` sometimes return NaN?",
              "options": [
                "Large positive logits cause floating point overflow",
                "Large negative logits cause underflow",
                "Logits cannot be zero",
                "PyTorch does not support exponentiation"
              ],
              "solution": "Large positive logits cause floating point overflow"
            },
            {
              "type": "gap_fill",
              "instruction": "PyTorch's `cross_entropy` subtracts the ___ of the logits internally for stability.",
              "template": "It subtracts the ___ value.",
              "solution": "maximum"
            },
            {
              "type": "matching",
              "instruction": "Match the PyTorch function to its behavior.",
              "pairs": [
                {
                  "item": "F.cross_entropy",
                  "match": "Softmax + Negative Log Likelihood (Stable)"
                },
                {
                  "item": "F.nll_loss",
                  "match": "Expects Log-Probabilities as input"
                },
                {
                  "item": "F.softmax",
                  "match": "Converts logits to probabilities (0-1)"
                }
              ]
            }
          ]
        },
        {
          "id": "mlp_ch6_training_loop",
          "title": "The Training Loop & Minibatching",
          "video_id": "TCH_1BHY58I",
          "description": "Construct the training loop, implement Minibatching to speed up iterations, and verify by overfitting a small batch.",
          "start": 2276,
          "end": 2740,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Overfitting as a sanity check",
            "Implementing Minibatch Gradient Descent",
            "Selecting random indices for batches"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the steps inside the training loop.",
              "lines": [
                "Construct minibatch (ix)",
                "Forward pass (logits, loss)",
                "Zero gradients",
                "Backward pass",
                "Update parameters"
              ],
              "solution": [
                "Construct minibatch (ix)",
                "Forward pass (logits, loss)",
                "Zero gradients",
                "Backward pass",
                "Update parameters"
              ]
            },
            {
              "type": "debug",
              "instruction": "The model trains very slowly. What is missing?",
              "code": "# Training on full X and Y every step\nlogits = model(X)\nloss = F.cross_entropy(logits, Y)",
              "solution": "Use minibatches (random subset of X, Y)",
              "hints": [
                "Process a small batch (e.g., 32) per step."
              ]
            }
          ]
        },
        {
          "id": "mlp_ch7_learning_rate",
          "title": "Finding the Optimal Learning Rate",
          "video_id": "TCH_1BHY58I",
          "description": "Learn the technique of sweeping learning rates exponentially to find the optimal value ('valley' of the loss plot).",
          "start": 2740,
          "end": 3200,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Using `torch.linspace` vs `torch.logspace`",
            "Interpreting the Loss vs LR plot",
            "Learning Rate Decay schedules"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "To search for learning rates, we step linearly through the ___.",
              "template": "lre = torch.linspace(-3, 0, 1000) # stepped through ___",
              "solution": "exponents"
            },
            {
              "type": "quiz",
              "instruction": "Where is the optimal learning rate usually found on the Loss vs LR plot?",
              "options": [
                "In the 'valley' just before the loss explodes",
                "At the very lowest value (0.00001)",
                "At the very highest value",
                "It is always 0.1"
              ],
              "solution": "In the 'valley' just before the loss explodes"
            }
          ]
        },
        {
          "id": "mlp_ch8_splits",
          "title": "Train, Validation, Test Splits",
          "video_id": "TCH_1BHY58I",
          "description": "Split the dataset into three parts to detect overfitting and properly evaluate model generalization.",
          "start": 3200,
          "end": 3649,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Standard split ratios (80/10/10)",
            "Purpose of Validation (Hyperparam tuning) vs Test (Final eval)",
            "Detecting Overfitting vs Underfitting"
          ],
          "tasks": [
            {
              "type": "matching",
              "instruction": "Match the dataset split to its purpose.",
              "pairs": [
                {
                  "item": "Train Set",
                  "match": "Optimize parameters (Gradient Descent)"
                },
                {
                  "item": "Validation/Dev Set",
                  "match": "Tune hyperparameters (Size, LR)"
                },
                {
                  "item": "Test Set",
                  "match": "Final evaluation (Only once)"
                }
              ]
            },
            {
              "type": "quiz",
              "instruction": "If Training Loss is low but Validation Loss is high, what is happening?",
              "options": [
                "Overfitting",
                "Underfitting",
                "Perfect Fit",
                "Data Leakage"
              ],
              "solution": "Overfitting"
            }
          ]
        },
        {
          "id": "mlp_ch9_scaling",
          "title": "Scaling Up & Visualizing",
          "video_id": "TCH_1BHY58I",
          "description": "Increase model capacity (hidden layer, embedding size) and visualize the learned character embeddings to interpret results.",
          "start": 3649,
          "end": 4495,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Bottlenecks in model capacity",
            "Interpreting 2D embedding plots (clustering)",
            "Sampling from the trained MLP"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "When visualizing embeddings, what does it mean if 'a', 'e', 'i', 'o', 'u' are clustered together?",
              "options": [
                "The model learned they are interchangeable in similar contexts",
                "The model is broken",
                "The random initialization was lucky",
                "They appear next to each other in the dataset"
              ],
              "solution": "The model learned they are interchangeable in similar contexts"
            },
            {
              "type": "debug",
              "instruction": "The validation loss stopped improving even though we increased hidden size. What might be the bottleneck?",
              "code": "# Hidden Layer: 300 neurons\n# Embedding Size: 2",
              "solution": "Embedding Size is too small (bottleneck)",
              "hints": [
                "You are cramming too much info into 2 dimensions."
              ]
            }
          ]
        }
      ]
    },{
      "id": "course_makemore_activations",
      "title": "Deep Neural Net Internals: Activations, Gradients & BatchNorm",
      "thumbnail": "assets/makemore-part3-thumbnail.jpg",
      "description": "Dive deep into the training dynamics of MLPs. Learn to diagnose and fix initialization issues, vanishing gradients, and saturated neurons using Kaiming Init and Batch Normalization.",
      "sequence_order": 5,
      "chapters": [
        {
          "id": "makemore_ch1_initialization",
          "title": "Fixing Initialization: The Hockey Stick",
          "video_id": "P6sfmUTpUmc",
          "description": "Diagnose why the initial loss is too high and fix it by squashing the output layer weights, preventing the 'hockey stick' loss curve.",
          "start": 259,
          "end": 779,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Calculating the expected initial loss (-log(1/vocab_size))",
            "Identifying the 'Hockey Stick' loss phenomenon",
            "Squashing output logits to ensure uniform probability at start"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If we have 27 characters and predict them with equal probability, what is the expected loss? (Round to 2 decimals)",
              "solution": "3.30",
              "hints": [
                "-log(1/27)"
              ]
            },
            {
              "type": "debug",
              "instruction": "The model starts with a huge loss (e.g., 27.0). Fix the output layer initialization.",
              "code": "W2 = torch.randn((n_hidden, vocab_size))\nb2 = torch.randn(vocab_size)",
              "solution": "W2 = torch.randn((n_hidden, vocab_size)) * 0.01\nb2 = torch.zeros(vocab_size)",
              "hints": [
                "Biases should be zero.",
                "Weights should be small to ensure logits are near zero."
              ]
            },
            {
              "type": "quiz",
              "instruction": "What causes the 'hockey stick' shape in the loss curve?",
              "options": [
                "The model is learning 'easy' gains by reducing over-confidence",
                "The learning rate is too high",
                "The dataset is shuffled incorrectly",
                "The batch size is too small"
              ],
              "solution": "The model is learning 'easy' gains by reducing over-confidence"
            }
          ]
        },
        {
          "id": "makemore_ch2_tanh_saturation",
          "title": "Vanishing Gradients & Saturated Tanh",
          "video_id": "P6sfmUTpUmc",
          "description": "Analyze the distribution of hidden states to find saturated neurons and understand why they kill gradients.",
          "start": 779,
          "end": 1673,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Visualizing activation histograms",
            "Understanding the derivative of Tanh: (1 - t^2)",
            "Defining 'Dead Neurons' (always saturated)"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If a Tanh neuron outputs 1.0, what is the local gradient that passes through it?",
              "solution": "0",
              "hints": [
                "Gradient = 1 - t^2"
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "If a neuron's activation is always in the flat tails of the nonlinearity, it is considered a ___ neuron.",
              "template": "This is a ___ neuron.",
              "solution": "dead"
            },
            {
              "type": "matching",
              "instruction": "Match the activation value to the gradient flow.",
              "pairs": [
                {
                  "item": "Tanh output = 0",
                  "match": "Gradient passes through (1.0)"
                },
                {
                  "item": "Tanh output = 1",
                  "match": "Gradient is killed (0.0)"
                },
                {
                  "item": "ReLU input < 0",
                  "match": "Gradient is killed (0.0)"
                }
              ]
            }
          ]
        },
        {
          "id": "makemore_ch3_kaiming_init",
          "title": "Kaiming Initialization",
          "video_id": "P6sfmUTpUmc",
          "description": "Derive the Kaiming Initialization math to preserve the variance of activations throughout deep networks.",
          "start": 1673,
          "end": 2440,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Why standard deviation expands during matrix multiplication",
            "The Kaiming Init Formula: gain / sqrt(fan_in)",
            "Different gains for ReLU vs Tanh"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "For a linear layer with 100 inputs (fan_in), what should be the standard deviation of the weights to preserve variance (Gain=1)?",
              "solution": "0.1",
              "hints": [
                "1 / sqrt(100)"
              ]
            },
            {
              "type": "matching",
              "instruction": "Match the non-linearity to its recommended Kaiming gain.",
              "pairs": [
                {
                  "item": "Tanh",
                  "match": "5/3"
                },
                {
                  "item": "ReLU",
                  "match": "sqrt(2)"
                },
                {
                  "item": "Linear",
                  "match": "1"
                }
              ]
            },
            {
              "type": "debug",
              "instruction": "This initialization causes activations to shrink. Fix the formula.",
              "code": "std = 1.0 / fan_in",
              "solution": "std = 1.0 / (fan_in ** 0.5)",
              "hints": [
                "You need the square root of fan_in."
              ]
            }
          ]
        },
        {
          "id": "makemore_ch4_batchnorm",
          "title": "Batch Normalization",
          "video_id": "P6sfmUTpUmc",
          "description": "Implement Batch Normalization from scratch to force activations to be Gaussian, and handle the training vs. inference discrepancy.",
          "start": 2440,
          "end": 3787,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Normalizing inputs by (x - mean) / std",
            "The role of Scale (Gamma) and Shift (Beta)",
            "Using Running Buffers for inference",
            "Why Bias in the previous layer is unnecessary"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the operations inside a Batch Norm layer.",
              "lines": [
                "Calculate Batch Mean",
                "Calculate Batch Variance",
                "Normalize: (x - mean) / sqrt(var + eps)",
                "Scale and Shift: x_hat * gamma + beta"
              ],
              "solution": [
                "Calculate Batch Mean",
                "Calculate Batch Variance",
                "Normalize: (x - mean) / sqrt(var + eps)",
                "Scale and Shift: x_hat * gamma + beta"
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why do we set `bias=False` in a Linear layer if it's immediately followed by BatchNorm?",
              "options": [
                "BatchNorm's centering subtracts the bias anyway, making it useless",
                "It causes a dimension mismatch error",
                "It makes the model overfit",
                "PyTorch requires it"
              ],
              "solution": "BatchNorm's centering subtracts the bias anyway, making it useless"
            },
            {
              "type": "gap_fill",
              "instruction": "During training, BN uses batch statistics. During inference, it uses ___ statistics.",
              "template": "It uses ___ buffers.",
              "solution": "running"
            }
          ]
        },
        {
          "id": "makemore_ch5_pytorch_modules",
          "title": "PyTorch-ifying the Code",
          "video_id": "P6sfmUTpUmc",
          "description": "Refactor the manual code into clean Classes (Linear, BatchNorm1d, Tanh) that mimic the PyTorch API structure.",
          "start": 4715,
          "end": 5211,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Understanding `__call__` vs `forward`",
            "Distinguishing `parameters` (learnable) from `buffers` (running stats)",
            "Managing the `training` flag"
          ],
          "tasks": [
            {
              "type": "matching",
              "instruction": "Classify the variable as a Parameter or Buffer in BatchNorm.",
              "pairs": [
                {
                  "item": "gamma (scale)",
                  "match": "Parameter (Gradient descent)"
                },
                {
                  "item": "running_mean",
                  "match": "Buffer (Momentum update)"
                },
                {
                  "item": "beta (shift)",
                  "match": "Parameter (Gradient descent)"
                }
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "We use the decorator `torch.___` to disable gradient tracking when updating running stats.",
              "template": "torch.___",
              "solution": "no_grad"
            },
            {
              "type": "debug",
              "instruction": "The model is failing to learn. The training loop isn't updating the weights. Fix the loop.",
              "code": "for p in model.parameters():\n    p.data += -lr * p.grad",
              "solution": "for p in model.parameters():\n    p.data += -lr * p.grad",
              "hints": [
                "Wait, this code is actually correct for SGD. The trick is: did you zero_grad? Ensure p.grad is not None."
              ]
            }
          ]
        },
        {
          "id": "makemore_ch6_diagnostics",
          "title": "Diagnostic Visualizations",
          "video_id": "P6sfmUTpUmc",
          "description": "Visualize activation histograms, gradient distributions, and the 'Update-to-Data' ratio to diagnose training health.",
          "start": 5211,
          "end": 6394,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Detecting saturation via forward pass histograms",
            "Detecting exploding/vanishing gradients",
            "The ideal Update-to-Data ratio (~1e-3)"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If the weight value is 1.0 and the update step is 0.001, what is the Update-to-Data ratio (log10)?",
              "solution": "-3",
              "hints": [
                "log10(0.001 / 1.0)"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What does an Update-to-Data ratio of -1.0 (log10) imply?",
              "options": [
                "The updates are too aggressive/large",
                "The learning rate is too low",
                "The model is not learning",
                "This is the ideal ratio"
              ],
              "solution": "The updates are too aggressive/large"
            },
            {
              "type": "matching",
              "instruction": "Match the visualization to the problem it reveals.",
              "pairs": [
                {
                  "item": "Histogram concentrated at -1 and 1",
                  "match": "Saturated Tanh"
                },
                {
                  "item": "Histogram shrinking to 0 width at deep layers",
                  "match": "Vanishing Gradients"
                },
                {
                  "item": "Update Ratio < -5",
                  "match": "Learning Rate too low"
                }
              ]
            }
          ]
        }
      ]
    },
 {
      "id": "course_makemore_backprop",
      "title": "Becoming a Backprop Ninja",
      "thumbnail": "assets/makemore-backprop-thumbnail.jpg",
      "description": "Master manual backpropagation by deriving gradients for tensors, batch normalization, and cross-entropy loss from scratch, without using PyTorch's autograd.",
      "sequence_order": 6,
      "chapters": [
        {
          "id": "backprop_ch1_motivation",
          "title": "Why Manual Backprop Matters",
          "video_id": "q8SA3rM6ckI",
          "description": "Understand why backpropagation is a 'leaky abstraction' and why understanding the internals is crucial for debugging neural networks.",
          "start": 0,
          "end": 446,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Recognizing backpropagation as a leaky abstraction",
            "Identifying common pitfalls like exploding/vanishing gradients",
            "Understanding the historical context of manual gradient calculation"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why is relying solely on `loss.backward()` considered risky?",
              "options": [
                "It can hide subtle bugs like vanishing gradients or incorrect graph construction",
                "It is slower than manual calculation",
                "It is deprecated in modern PyTorch",
                "It only works on CPUs"
              ],
              "solution": "It can hide subtle bugs like vanishing gradients or incorrect graph construction"
            },
            {
              "type": "matching",
              "instruction": "Match the concept to the issue it causes.",
              "pairs": [
                {
                  "item": "Saturated Tanh",
                  "match": "Vanishing Gradients"
                },
                {
                  "item": "Dead ReLU",
                  "match": "Zero Gradients forever"
                },
                {
                  "item": "Clipping Loss instead of Gradients",
                  "match": "Ignored outliers"
                }
              ]
            }
          ]
        },
        {
          "id": "backprop_ch2_atomic_gradients",
          "title": "Backprop through Atomic Operations",
          "video_id": "q8SA3rM6ckI",
          "description": "Manually derive gradients for basic tensor operations like Log, View, and Matrix Multiplication by checking shapes and using the Chain Rule.",
          "start": 781,
          "end": 3122,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Calculating gradients for `log(x)` (1/x)",
            "Handling gradients for broadcasted operations (Summing over dimensions)",
            "Deriving gradients for Matrix Multiplication (Transpose rules)"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If y = log(x), what is dy/dx?",
              "solution": "1/x",
              "hints": [
                "Derivative of natural log"
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "When a tensor is broadcasted (replicated) in the forward pass, we must ___ the gradients in the backward pass.",
              "template": "We must ___ over the broadcasted dimension.",
              "solution": "sum"
            },
            {
              "type": "debug",
              "instruction": "The gradient shape for matrix multiplication `C = A @ B` is wrong. Fix `dA`.",
              "code": "# dC is the incoming gradient\ndA = dC @ B  # Wrong shape",
              "solution": "dA = dC @ B.T",
              "hints": [
                "Check the dimensions. You need to transpose B."
              ]
            }
          ]
        },
        {
          "id": "backprop_ch3_cross_entropy_analytic",
          "title": "Analytic Gradient of Cross Entropy",
          "video_id": "q8SA3rM6ckI",
          "description": "Simplify the backpropagation of the entire Cross Entropy Loss into a single, elegant mathematical expression: `softmax(logits) - 1` at the correct label.",
          "start": 5191,
          "end": 5797,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Deriving the gradient of Softmax + Cross Entropy",
            "The simplified formula: `probs[target] - 1`",
            "Understanding the 'Push/Pull' force of gradients"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If the predicted probability for the correct class is `p` and the target is 1, what is the gradient w.r.t the logit?",
              "solution": "p - 1",
              "hints": [
                "Softmax gradient simplification"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the intuitive interpretation of the cross-entropy gradient?",
              "options": [
                "It pulls up the probability of the correct class and pushes down others",
                "It always pushes probabilities to 0.5",
                "It ignores incorrect classes",
                "It squares the error"
              ],
              "solution": "It pulls up the probability of the correct class and pushes down others"
            },
            {
              "type": "gap_fill",
              "instruction": "The gradient of Cross Entropy Loss with respect to logits is essentially `softmax(logits)` minus the ___ vector.",
              "template": "Minus the ___ vector.",
              "solution": "one_hot"
            }
          ]
        },
        {
          "id": "backprop_ch4_batchnorm_backward",
          "title": "Backprop through Batch Normalization",
          "video_id": "q8SA3rM6ckI",
          "description": "Derive the complex gradient for the Batch Normalization layer, accounting for the dependency of the mean and variance on the input batch.",
          "start": 5802,
          "end": 6542,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Understanding the computational graph of BatchNorm",
            "Handling the multiple paths of influence (x -> mean, x -> var, x -> normalize)",
            "Simplifying the final analytical expression"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the dependencies for backpropagating through BatchNorm input `x`.",
              "lines": [
                "Gradient from normalized output (dx_hat)",
                "Gradient contribution via Mean (dmu)",
                "Gradient contribution via Variance (dvar)",
                "Sum all contributions to get dx"
              ],
              "solution": [
                "Gradient from normalized output (dx_hat)",
                "Gradient contribution via Mean (dmu)",
                "Gradient contribution via Variance (dvar)",
                "Sum all contributions to get dx"
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why is the gradient through the mean not zero?",
              "options": [
                "Because changing an input `x` changes the batch mean, which affects all other normalized outputs",
                "Because the mean is a constant",
                "It is zero actually",
                "Because of the epsilon term"
              ],
              "solution": "Because changing an input `x` changes the batch mean, which affects all other normalized outputs"
            },
            {
              "type": "matching",
              "instruction": "Match the term to its role in BatchNorm backprop.",
              "pairs": [
                {
                  "item": "dmu",
                  "match": "Gradient of the batch mean"
                },
                {
                  "item": "dvar",
                  "match": "Gradient of the batch variance"
                },
                {
                  "item": "dx_hat",
                  "match": "Gradient of the normalized input"
                }
              ]
            }
          ]
        },
        {
          "id": "backprop_ch5_full_implementation",
          "title": "Putting It All Together",
          "video_id": "q8SA3rM6ckI",
          "description": "Assemble the manual backward pass for the entire MLP, replace `loss.backward()`, and verify correct training results.",
          "start": 6602,
          "end": 6864,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Replacing autograd with manual gradients",
            "Verifying gradients against PyTorch (numerical check)",
            "Achieving identical training loss with manual backprop"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The manual training loop is not updating parameters. What is missing?",
              "code": "# Forward pass done, grads calculated manually\nfor p in parameters:\n    # Missing line",
              "solution": "p.data += -lr * p.grad",
              "hints": [
                "You need to apply the SGD update rule using the manually calculated gradients."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "To check our manual gradients, we compare them against `p.___` from PyTorch's autograd.",
              "template": "Compare against `p.___`",
              "solution": "grad"
            },
            {
              "type": "quiz",
              "instruction": "Does manual backprop require the `with torch.no_grad():` context?",
              "options": [
                "Yes, because we don't want PyTorch to build a graph for our manual update steps",
                "No, it's automatic",
                "Only for the forward pass",
                "Only for the loss calculation"
              ],
              "solution": "Yes, because we don't want PyTorch to build a graph for our manual update steps"
            }
          ]
        }
      ]
    },
   {
      "id": "course_makemore_wavenet",
      "title": "Building WaveNet: Hierarchical Language Models",
      "thumbnail": "assets/makemore-part5-thumbnail.jpg",
      "description": "Upgrade the MLP language model into a hierarchical WaveNet architecture. Learn to manage high-dimensional tensors, implement custom PyTorch modules, and debug subtle batch normalization errors.",
      "sequence_order": 7,
      "chapters": [
        {
          "id": "wavenet_ch1_pytorchification",
          "title": "Refactoring to PyTorch-Style Modules",
          "video_id": "t3YJ5hKiMQ0",
          "description": "Refactor the raw code into clean, reusable classes (Embedding, Flatten, Sequential) that mimic the `torch.nn` API structure.",
          "start": 556,
          "end": 1031,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Structuring code into `nn.Module`-like classes",
            "Implementing `Sequential` containers for layer stacking",
            "Replacing manual shape handling with modular layers"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "In a PyTorch-style module, parameters are initialized in `__init__` and the computation is defined in ___.",
              "template": "computation is defined in the ___ method.",
              "solution": "__call__"
            },
            {
              "type": "matching",
              "instruction": "Match the custom class to its PyTorch equivalent.",
              "pairs": [
                {
                  "item": "Flatten",
                  "match": "torch.nn.Flatten"
                },
                {
                  "item": "Embedding",
                  "match": "torch.nn.Embedding"
                },
                {
                  "item": "Sequential",
                  "match": "torch.nn.Sequential"
                }
              ]
            },
            {
              "type": "parsons",
              "instruction": "Arrange the logic for a simple Sequential container's forward pass.",
              "lines": [
                "def __call__(self, x):",
                "  for layer in self.layers:",
                "    x = layer(x)",
                "  return x"
              ],
              "solution": [
                "def __call__(self, x):",
                "  for layer in self.layers:",
                "    x = layer(x)",
                "  return x"
              ]
            }
          ]
        },
        {
          "id": "wavenet_ch2_high_dim_matmul",
          "title": "High-Dimensional Matrix Multiplication",
          "video_id": "t3YJ5hKiMQ0",
          "description": "Discover how PyTorch's matrix multiplication broadcasts over extra dimensions, enabling the parallel processing of sequence groups required for WaveNet.",
          "start": 1296,
          "end": 1602,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Understanding how `torch.matmul` handles 3D+ tensors",
            "The concept of 'Batch Dimensions' in linear layers",
            "Preparing data shapes for hierarchical fusion"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "Input shape is (4, 5, 80). Linear layer weights are (80, 200). What is the output shape?",
              "solution": "(4, 5, 200)",
              "hints": [
                "Matrix multiplication applies to the last dimension.",
                "Dimensions (4, 5) are treated as batch dimensions."
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why is it beneficial that Linear layers work on high-dimensional tensors?",
              "options": [
                "It allows us to process multiple groups of characters in parallel within a single example",
                "It automatically compresses the data",
                "It reduces the memory usage of weights",
                "It increases the learning rate"
              ],
              "solution": "It allows us to process multiple groups of characters in parallel within a single example"
            },
            {
              "type": "debug",
              "instruction": "The matrix multiplication is failing due to a shape mismatch. Input: (B, T, C). Weights: (C, H). Identify the fix.",
              "code": "# Input x shape: (32, 8, 10)\n# Weights w shape: (20, 10)\nout = x @ w",
              "solution": "Weights must be transposed to (10, 20) or match input dimension C.",
              "hints": [
                "The last dimension of Input must match the first dimension of Weights."
              ]
            }
          ]
        },
        {
          "id": "wavenet_ch3_flatten_consecutive",
          "title": "Implementing WaveNet: FlattenConsecutive",
          "video_id": "t3YJ5hKiMQ0",
          "description": "Build the `FlattenConsecutive` layer to fuse adjacent characters hierarchically, transforming the flat MLP into a tree-structured WaveNet.",
          "start": 1602,
          "end": 2261,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Implementing hierarchical information fusion",
            "Reshaping tensors using `.view()` for grouping",
            "Calculating new tensor shapes after flattening groups"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "Input: (4, 8, 10). We want to fuse consecutive pairs (factor 2). What is the reshaped target dimensions before squeezing?",
              "solution": "(4, 4, 20)",
              "hints": [
                "Batch stays 4.",
                "Sequence length 8 becomes 4 groups of 2.",
                "Channels 10 becomes 2*10 = 20."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "To fuse adjacent elements, we use `.view` to split the sequence dimension and then flatten the groups into the ___ dimension.",
              "template": "flatten into the ___ dimension.",
              "solution": "channel"
            },
            {
              "type": "parsons",
              "instruction": "Construct a 3-layer WaveNet block.",
              "lines": [
                "FlattenConsecutive(2)",
                "Linear(n_embd * 2, n_hidden)",
                "BatchNorm1d(n_hidden)",
                "Tanh()"
              ],
              "solution": [
                "FlattenConsecutive(2)",
                "Linear(n_embd * 2, n_hidden)",
                "BatchNorm1d(n_hidden)",
                "Tanh()"
              ]
            }
          ]
        },
        {
          "id": "wavenet_ch4_batchnorm_bug",
          "title": "Debugging BatchNorm Dimensions",
          "video_id": "t3YJ5hKiMQ0",
          "description": "Identify and fix a critical bug in `BatchNorm1d` where statistics were calculated incorrectly for 3D inputs, learning to aggregate over spatial dimensions.",
          "start": 2330,
          "end": 2721,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Diagnosing shape mismatches in running statistics",
            "The difference between calculating mean over `dim=0` vs `dim=(0,1)`",
            "Handling variable input dimensions (2D vs 3D) in custom layers"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The BatchNorm layer is maintaining separate means for each position in the sequence, which is wrong. Fix the mean calculation.",
              "code": "# Input x shape: (32, 4, 64)\n# Current bug: mean over batch only\nmean = x.mean(0, keepdim=True)",
              "solution": "mean = x.mean((0, 1), keepdim=True)",
              "hints": [
                "We want one mean per channel, averaging over both Batch (0) and Sequence (1)."
              ]
            },
            {
              "type": "quiz",
              "instruction": "If we don't fix the BatchNorm bug, what happens to the running statistics?",
              "options": [
                "They have the wrong shape (including sequence length) and fail to generalize",
                "They become all zeros",
                "The code crashes immediately",
                "Nothing, it works fine"
              ],
              "solution": "They have the wrong shape (including sequence length) and fail to generalize"
            },
            {
              "type": "matching",
              "instruction": "Match the input tensor rank to the dimensions to average over for BatchNorm.",
              "pairs": [
                {
                  "item": "2D Tensor (N, C)",
                  "match": "dim=0"
                },
                {
                  "item": "3D Tensor (N, L, C)",
                  "match": "dim=(0, 1)"
                }
              ]
            }
          ]
        }
      ]
    },{
      "id": "course_llm_tokenization",
      "title": "LLM Tokenization: From Byte Pair Encoding to GPT-4",
      "thumbnail": "assets/tokenization-thumbnail.jpg",
      "description": "A deep dive into how Large Language Models process text. Learn about Unicode, Byte Pair Encoding (BPE), and the specific tokenizers used in GPT-2, GPT-4, and Llama.",
      "sequence_order": 8,
      "chapters": [
        {
          "id": "tokenization_ch1_intro",
          "title": "The Hidden Root of LLM Issues",
          "video_id": "zduSFxRajkE",
          "description": "Understand why tokenization is the fundamental atom of LLMs and how it causes common problems like spelling errors, math failures, and poor non-English performance.",
          "start": 0,
          "end": 896,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Defining a Token as the fundamental unit of LLMs",
            "Identifying tokenization-induced failures (spelling, arithmetic)",
            "Visualizing token chunks using tools like Tiktokenizer"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why are LLMs often bad at simple tasks like reversing a string or counting letters?",
              "options": [
                "They see text as chunks (tokens), not individual characters",
                "They are not trained on enough text",
                "They do not have enough memory",
                "They operate on raw binary data"
              ],
              "solution": "They see text as chunks (tokens), not individual characters"
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the missing word.",
              "template": "Tokenization is the process of translating strings into sequences of ___.",
              "solution": "integers"
            },
            {
              "type": "matching",
              "instruction": "Match the text to its likely tokenization behavior in GPT-2.",
              "pairs": [
                {
                  "item": "Common word ('the')",
                  "match": "Single Token"
                },
                {
                  "item": "Rare string ('SolidGoldMagikarp')",
                  "match": "Multiple Tokens (or broken)"
                },
                {
                  "item": "Python indentation",
                  "match": "Many individual space tokens"
                }
              ]
            }
          ]
        },
        {
          "id": "tokenization_ch2_unicode",
          "title": "Unicode, UTF-8, and Bytes",
          "video_id": "zduSFxRajkE",
          "description": "Explore how computers represent text, from Python strings to Unicode code points and UTF-8 byte encodings.",
          "start": 896,
          "end": 1430,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Difference between Python strings and Bytes",
            "How UTF-8 encodes Unicode code points",
            "Why raw UTF-8 bytes result in sequences that are too long"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If we used a naive tokenizer based purely on raw byte values (UTF-8), what would be the vocabulary size?",
              "solution": "256",
              "hints": [
                "A byte has 8 bits. 2^8."
              ]
            },
            {
              "type": "debug",
              "instruction": "Why does this code fail or return a different length?",
              "code": "len(''.encode('utf-8'))",
              "solution": "It returns 4 bytes, while the string length is 1 character.",
              "hints": [
                "Emojis are often 4 bytes in UTF-8."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "UTF-8 is a ___ length encoding, meaning characters can use 1 to 4 bytes.",
              "template": "variable",
              "solution": "variable"
            }
          ]
        },
        {
          "id": "tokenization_ch3_bpe_algo",
          "title": "Byte Pair Encoding (BPE) Algorithm",
          "video_id": "zduSFxRajkE",
          "description": "Learn the BPE algorithm: iteratively finding the most frequent pair of bytes and merging them to create a new token.",
          "start": 1430,
          "end": 2360,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Calculating pair statistics in a sequence",
            "Merging the most frequent pair",
            "Understanding compression ratio via merging"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the steps of one iteration of BPE training.",
              "lines": [
                "Count frequency of all adjacent pairs",
                "Identify the most frequent pair",
                "Mint a new token ID for this pair",
                "Replace all occurrences of the pair with the new token"
              ],
              "solution": [
                "Count frequency of all adjacent pairs",
                "Identify the most frequent pair",
                "Mint a new token ID for this pair",
                "Replace all occurrences of the pair with the new token"
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If you start with 256 byte tokens and perform 20 merges, what is your final vocabulary size?",
              "solution": "276",
              "hints": [
                "256 + 20"
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the primary goal of BPE merging?",
              "options": [
                "To compress the sequence length",
                "To encrypt the data",
                "To remove non-English characters",
                "To increase the sequence length"
              ],
              "solution": "To compress the sequence length"
            }
          ]
        },
        {
          "id": "tokenization_ch4_implementation",
          "title": "Implementing Encode and Decode",
          "video_id": "zduSFxRajkE",
          "description": "Write the Python code to encode text into token IDs and decode token IDs back into text, handling edge cases like invalid UTF-8.",
          "start": 2360,
          "end": 3456,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Building the 'vocab' dictionary from merges",
            "Implementing the `decode` function using `errors='replace'`",
            "Implementing the `encode` function using a while loop"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The decode function crashes on some AI-generated tokens. Fix the error handling.",
              "code": "text = bytes(tokens).decode('utf-8')",
              "solution": "text = bytes(tokens).decode('utf-8', errors='replace')",
              "hints": [
                "Not all byte sequences are valid UTF-8."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "To decode, we concatenate the bytes of each token and then decode them as ___.",
              "template": "decode them as ___.",
              "solution": "utf-8"
            },
            {
              "type": "matching",
              "instruction": "Match the data structure to its purpose.",
              "pairs": [
                {
                  "item": "merges",
                  "match": "Maps pairs of IDs to a new ID (encoding)"
                },
                {
                  "item": "vocab",
                  "match": "Maps IDs to Bytes (decoding)"
                }
              ]
            }
          ]
        },
        {
          "id": "tokenization_ch5_regex",
          "title": "Regex Splitting and GPT-2",
          "video_id": "zduSFxRajkE",
          "description": "Understand why GPT-2 uses Regex to pre-split text into categories (letters, numbers, punctuation) to prevent cross-category merges.",
          "start": 3456,
          "end": 4298,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "The role of Regex in forcing splits",
            "Preventing 'dog.' from being a single token",
            "Handling contractions and punctuation"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why does GPT-2 use a regex pattern before BPE?",
              "options": [
                "To prevent merging distinct categories like words and punctuation",
                "To remove emojis",
                "To convert everything to uppercase",
                "To count the number of words"
              ],
              "solution": "To prevent merging distinct categories like words and punctuation"
            },
            {
              "type": "matching",
              "instruction": "Match the regex component to what it captures.",
              "pairs": [
                {
                  "item": "'s|'t|'re",
                  "match": "Contractions"
                },
                {
                  "item": "\\p{L}+",
                  "match": "Letters (Words)"
                },
                {
                  "item": "\\p{N}+",
                  "match": "Numbers"
                }
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "The GPT-2 regex ensures that 'dog' and 'dog?' are processed ___.",
              "template": "processed ___.",
              "solution": "separately"
            }
          ]
        },
        {
          "id": "tokenization_ch6_tiktoken",
          "title": "Tiktoken and GPT-4",
          "video_id": "zduSFxRajkE",
          "description": "Explore OpenAI's `tiktoken` library, the improvements in GPT-4's tokenizer (cl100k_base), and the concept of Special Tokens.",
          "start": 4298,
          "end": 5322,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Using the `tiktoken` library",
            "Differences between GPT-2 (50k vocab) and GPT-4 (100k vocab)",
            "Handling Special Tokens like `<|endoftext|>`"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "GPT-4's vocabulary size is roughly how many tokens?",
              "solution": "100277",
              "hints": [
                "It's around 100k."
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the purpose of `<|endoftext|>`?",
              "options": [
                "To signal the boundary between unrelated documents",
                "To end the Python script",
                "To reset the weights",
                "It is a bug"
              ],
              "solution": "To signal the boundary between unrelated documents"
            },
            {
              "type": "gap_fill",
              "instruction": "GPT-4's tokenizer handles ___ better by merging more spaces into single tokens.",
              "template": "handles ___ code better.",
              "solution": "python"
            }
          ]
        },
        {
          "id": "tokenization_ch7_sentencepiece",
          "title": "SentencePiece and Llama",
          "video_id": "zduSFxRajkE",
          "description": "Examine the SentencePiece library used by models like Llama, which operates on Unicode code points and includes byte fallbacks.",
          "start": 5322,
          "end": 6701,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Difference between SentencePiece (Code Points) and Tiktoken (Bytes)",
            "The 'Byte Fallback' mechanism for unknown characters",
            "Trade-offs of Vocabulary Size"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "How does SentencePiece handle unknown characters if `byte_fallback` is True?",
              "options": [
                "It decomposes them into UTF-8 byte tokens",
                "It maps them to <UNK>",
                "It crashes",
                "It deletes them"
              ],
              "solution": "It decomposes them into UTF-8 byte tokens"
            },
            {
              "type": "matching",
              "instruction": "Match the library to its behavior.",
              "pairs": [
                {
                  "item": "Tiktoken",
                  "match": "Encodes to UTF-8 bytes first, then BPE"
                },
                {
                  "item": "SentencePiece",
                  "match": "BPE on Code Points, then Byte Fallback"
                }
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If you increase vocab size, the embedding layer parameters increase. If d_model=768 and you add 1000 tokens, how many parameters do you add?",
              "solution": "768000",
              "hints": [
                "768 * 1000"
              ]
            }
          ]
        },
        {
          "id": "tokenization_ch8_quirks",
          "title": "LLM Quirks & Security",
          "video_id": "zduSFxRajkE",
          "description": "Uncover the mysteries of 'SolidGoldMagikarp', trailing whitespace warnings, and why these issues are traced back to tokenization.",
          "start": 6701,
          "end": 7980,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Why 'SolidGoldMagikarp' breaks models (Untrained tokens)",
            "Trailing whitespace issues",
            "Why JSON is token-inefficient compared to YAML"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why did the 'SolidGoldMagikarp' token cause the model to crash or hallucinate?",
              "options": [
                "The token existed in the vocab but appeared 0 times in the training data",
                "It was a magic keyword hardcoded by developers",
                "It was too long",
                "It contained invalid UTF-8"
              ],
              "solution": "The token existed in the vocab but appeared 0 times in the training data"
            },
            {
              "type": "debug",
              "instruction": "Why might a prompt ending in a space cause performance degradation?",
              "code": "\"Tell me a story about \"",
              "solution": "The model expects ' about' (with space) as one token, but sees ' about' and ' ' separately.",
              "hints": [
                "Token boundaries matter."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "For structured data, ___ is often preferred over JSON because it uses fewer tokens.",
              "template": "___ is preferred.",
              "solution": "YAML"
            }
          ]
        }
      ]
    },{
      "id": "course_build_gpt_karpathy",
      "title": "Building GPT from Scratch",
      "thumbnail": "assets/gpt-thumbnail.jpg",
      "description": "Code a Generative Pre-trained Transformer (GPT) from the ground up using PyTorch. Start with a simple Bigram model and evolve it into a full Transformer with Self-Attention, Multi-Head Attention, and Residual connections.",
      "sequence_order": 9,
      "chapters": [
        {
          "id": "gpt_ch1_data_loader",
          "title": "Data Loading and Batching",
          "video_id": "kCc8FmEb1nY",
          "description": "Prepare the Tiny Shakespeare dataset for training. Implement character-level tokenization and create a robust data loader that generates batches of inputs (x) and targets (y) for supervised learning.",
          "start": 472,
          "end": 1331,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "Character-level tokenization (stoi/itos)",
            "Creating context windows (block_size)",
            "Batching data for efficient GPU training"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "The target `y` is the input `x` shifted by ___ to the right.",
              "template": "shifted by ___.",
              "solution": "one"
            },
            {
              "type": "math_challenge",
              "instruction": "If batch_size=4 and block_size=8, how many independent training examples are processed in one forward pass?",
              "solution": "32",
              "hints": [
                "4 rows * 8 time steps per row."
              ]
            },
            {
              "type": "debug",
              "instruction": "The data loader is crashing because the random offset `ix` is too large. Fix the range.",
              "code": "ix = torch.randint(len(data), (batch_size,))",
              "solution": "ix = torch.randint(len(data) - block_size, (batch_size,))",
              "hints": [
                "You need to leave room for the block_size."
              ]
            }
          ]
        },
        {
          "id": "gpt_ch2_bigram_baseline",
          "title": "The Bigram Language Model",
          "video_id": "kCc8FmEb1nY",
          "description": "Establish a baseline using a simple Bigram model where prediction depends only on the current token. Implement the embedding table and the training loop.",
          "start": 1331,
          "end": 2533,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Using `nn.Embedding` as a lookup table",
            "Calculating Cross Entropy Loss",
            "The training loop: zero_grad, backward, step"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "In a Bigram model, does the prediction depend on the entire history of tokens?",
              "options": [
                "No, only the immediate current token",
                "Yes, all previous tokens",
                "Yes, but only the last 3 tokens",
                "No, it predicts randomly"
              ],
              "solution": "No, only the immediate current token"
            },
            {
              "type": "parsons",
              "instruction": "Order the steps of a training iteration.",
              "lines": [
                "logits, loss = model(xb, yb)",
                "optimizer.zero_grad(set_to_none=True)",
                "loss.backward()",
                "optimizer.step()"
              ],
              "solution": [
                "logits, loss = model(xb, yb)",
                "optimizer.zero_grad(set_to_none=True)",
                "loss.backward()",
                "optimizer.step()"
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If vocab_size=65, what is the expected initial loss (-ln(1/65))? Round to 2 decimals.",
              "solution": "4.17",
              "hints": [
                "-ln(1/65)"
              ]
            }
          ]
        },
        {
          "id": "gpt_ch3_self_attention_math",
          "title": "The Math of Self-Attention",
          "video_id": "kCc8FmEb1nY",
          "description": "Derive the mathematical mechanism of self-attention. Learn how matrix multiplication with a lower-triangular mask achieves weighted aggregation of past context.",
          "start": 2533,
          "end": 3720,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "The trick: Matrix multiplication as weighted sum",
            "Masking future tokens with `-inf` (Tril)",
            "Softmax normalization for attention weights"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "We mask the upper triangular part of the attention matrix with ___ before softmax.",
              "template": "masked with ___.",
              "solution": "-inf"
            },
            {
              "type": "matching",
              "instruction": "Match the attention component to its role.",
              "pairs": [
                {
                  "item": "Query (Q)",
                  "match": "What I am looking for"
                },
                {
                  "item": "Key (K)",
                  "match": "What I contain"
                },
                {
                  "item": "Value (V)",
                  "match": "What I communicate"
                }
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If we have a sequence of length T=3, how many attention scores are calculated in total (before masking) for one head?",
              "solution": "9",
              "hints": [
                "T * T matrix"
              ]
            }
          ]
        },
        {
          "id": "gpt_ch4_implementing_head",
          "title": "Implementing a Single Attention Head",
          "video_id": "kCc8FmEb1nY",
          "description": "Code a `Head` class that implements scaled dot-product attention: Q, K, V projections, masking, softmax, and aggregation.",
          "start": 3720,
          "end": 4919,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Scaled Dot-Product Attention: (Q @ K.T) / sqrt(d_k)",
            "Why we divide by sqrt(head_size) (Variance preservation)",
            "Implementing the causal mask (Decoder-only)"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The attention scores are exploding. What is missing?",
              "code": "wei = q @ k.transpose(-2, -1)\nwei = wei.masked_fill(tril == 0, float('-inf'))",
              "solution": "wei = q @ k.transpose(-2, -1) * (C ** -0.5)",
              "hints": [
                "You forgot the scaling factor (1/sqrt(head_size))."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "In self-attention, the Query, Key, and Value all come from the same source ___.",
              "template": "Source ___.",
              "solution": "x"
            },
            {
              "type": "quiz",
              "instruction": "Why is it called 'Self-Attention'?",
              "options": [
                "Because Q, K, and V are produced from the same input source",
                "Because the model attends to itself in the mirror",
                "Because it ignores external inputs",
                "Because it only has one head"
              ],
              "solution": "Because Q, K, and V are produced from the same input source"
            }
          ]
        },
        {
          "id": "gpt_ch5_multi_head_attention",
          "title": "Multi-Head Attention",
          "video_id": "kCc8FmEb1nY",
          "description": "Upgrade the model to use Multi-Head Attention. Run multiple heads in parallel and concatenate their outputs to capture different types of relationships.",
          "start": 4919,
          "end": 5065,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Concatenating multiple head outputs",
            "Why multiple heads help (Parallel feature extraction)",
            "The projection layer (Linear) after concatenation"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "If embedding dimension is 32 and we have 4 heads, what is the head_size?",
              "solution": "8",
              "hints": [
                "32 / 4"
              ]
            },
            {
              "type": "parsons",
              "instruction": "Order the steps in Multi-Head Attention forward pass.",
              "lines": [
                "Run all heads in parallel",
                "Concatenate outputs along the channel dimension",
                "Apply final linear projection"
              ],
              "solution": [
                "Run all heads in parallel",
                "Concatenate outputs along the channel dimension",
                "Apply final linear projection"
              ]
            }
          ]
        },
        {
          "id": "gpt_ch6_transformer_block",
          "title": "The Transformer Block",
          "video_id": "kCc8FmEb1nY",
          "description": "Assemble the full Transformer Block: Multi-Head Attention, Feed-Forward Network, Residual Connections, and Layer Normalization.",
          "start": 5065,
          "end": 5869,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "The Feed-Forward Network (FFN) adds computation",
            "Residual Connections (Add) help deep training",
            "LayerNorm (Norm) stabilizes activations"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "The residual connection adds the input `x` back to the output of the sub-layer: `x = x + ___`.",
              "template": "x = x + ___",
              "solution": "sublayer(x)"
            },
            {
              "type": "matching",
              "instruction": "Match the component to its function.",
              "pairs": [
                {
                  "item": "Attention",
                  "match": "Communication (Aggregation)"
                },
                {
                  "item": "Feed-Forward",
                  "match": "Computation (Thinking)"
                },
                {
                  "item": "LayerNorm",
                  "match": "Normalization"
                }
              ]
            },
            {
              "type": "debug",
              "instruction": "The residual connection logic is wrong. Fix it.",
              "code": "x = self.sa(x)\nx = self.ffwd(x)",
              "solution": "x = x + self.sa(x)\nx = x + self.ffwd(x)",
              "hints": [
                "You are missing the addition (x + ...)."
              ]
            }
          ]
        },
        {
          "id": "gpt_ch7_scaling_up",
          "title": "Scaling to GPT",
          "video_id": "kCc8FmEb1nY",
          "description": "Scale up the model depth and width, add Dropout for regularization, and verify the final architecture matches the GPT-2 paper.",
          "start": 5869,
          "end": 6872,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Adding Dropout to prevent overfitting",
            "Scaling hyperparameters (n_layer, n_head, n_embd)",
            "The importance of Pre-Norm vs Post-Norm"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "What is the purpose of Dropout?",
              "options": [
                "To prevent overfitting by randomly zeroing activations",
                "To make the model faster",
                "To reduce the number of parameters",
                "To increase the learning rate"
              ],
              "solution": "To prevent overfitting by randomly zeroing activations"
            },
            {
              "type": "math_challenge",
              "instruction": "If the Feed-Forward inner dimension is 4 times the embedding dimension (384), what is the inner size?",
              "solution": "1536",
              "hints": [
                "384 * 4"
              ]
            }
          ]
        }
      ]
    },  {
      "id": "course_gpt2_repro",
      "title": "Reproducing GPT-2 (124M) from Scratch",
      "thumbnail": "assets/gpt2-repro-thumbnail.jpg",
      "description": "A comprehensive guide to building, training, and optimizing the GPT-2 124M model using PyTorch. Covers architecture implementation, weight loading, mixed-precision training, Flash Attention, and distributed training strategies.",
      "sequence_order": 10,
      "chapters": [
        {
          "id": "gpt2_ch1_architecture",
          "title": "GPT-2 Architecture & State Dict",
          "video_id": "l8pRSuU81PU",
          "description": "Analyze the GPT-2 124M architecture, map the HuggingFace checkpoint keys to PyTorch tensors, and define the model skeleton.",
          "start": 0,
          "end": 827,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Identifying GPT-2 124M hyperparameters (12 layers, 768 dim)",
            "Mapping State Dict keys to model components",
            "Understanding Positional vs. Token Embeddings"
          ],
          "tasks": [
            {
              "type": "matching",
              "instruction": "Match the tensor shape to the model component for GPT-2 (124M).",
              "pairs": [
                {
                  "item": "wte.weight",
                  "match": "(50257, 768)"
                },
                {
                  "item": "wpe.weight",
                  "match": "(1024, 768)"
                },
                {
                  "item": "h.0.attn.c_attn.weight",
                  "match": "(768, 2304)"
                }
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why is the attention weight shape (768, 2304)?",
              "options": [
                "It calculates Query, Key, and Value projections in a single matrix multiply",
                "It includes the feed-forward network weights",
                "It accounts for the vocabulary size",
                "It is a bug in the HuggingFace implementation"
              ],
              "solution": "It calculates Query, Key, and Value projections in a single matrix multiply"
            },
            {
              "type": "gap_fill",
              "instruction": "Fill in the missing class name for the token embeddings.",
              "template": "self.wte = nn.___(config.vocab_size, config.n_embd)",
              "solution": "Embedding"
            }
          ]
        },
        {
          "id": "gpt2_ch2_implementation",
          "title": "Implementing the GPT Module",
          "video_id": "l8pRSuU81PU",
          "description": "Implement the `GPT` class, including the MLP (FeedForward) and Causal Self-Attention blocks, matching the HuggingFace naming convention for weight loading.",
          "start": 827,
          "end": 2000,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Implementing the GELU activation function (approximate)",
            "Constructing the Causal Self-Attention mechanism",
            "Loading pre-trained weights into a custom module"
          ],
          "tasks": [
            {
              "type": "code_ordering",
              "instruction": "Order the operations inside the Transformer Block forward pass (Pre-Norm formulation).",
              "lines": [
                "x = x + self.attn(self.ln_1(x))",
                "x = x + self.mlp(self.ln_2(x))",
                "return x"
              ],
              "solution": [
                "x = x + self.attn(self.ln_1(x))",
                "x = x + self.mlp(self.ln_2(x))",
                "return x"
              ]
            },
            {
              "type": "debug",
              "instruction": "The attention mask logic is incorrect. Fix the `masked_fill` arguments to prevent attending to the future.",
              "code": "bias = torch.tril(torch.ones(config.block_size, config.block_size))\natt = att.masked_fill(bias == 1, float('-inf'))",
              "solution": "att = att.masked_fill(bias == 0, float('-inf'))",
              "hints": [
                "We want to mask positions where the lower triangular matrix is 0 (the future)."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "GPT-2 uses an approximation for the ___ activation function.",
              "template": "It uses the ___ activation.",
              "solution": "GELU"
            }
          ]
        },
        {
          "id": "gpt2_ch3_training_setup",
          "title": "Data Batching & Cross Entropy",
          "video_id": "l8pRSuU81PU",
          "description": "Set up the data loader to create batches of inputs and shifted targets, and implement the Cross Entropy loss calculation.",
          "start": 2750,
          "end": 3974,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Creating inputs (x) and targets (y) by shifting indices",
            "Flattening tensors for `F.cross_entropy`",
            "Calculating expected initial loss (-ln(1/vocab_size))"
          ],
          "tasks": [
            {
              "type": "debug",
              "instruction": "The shape passed to cross_entropy is causing an error. The logits are (B, T, C) and targets are (B, T). Fix the reshaping.",
              "code": "loss = F.cross_entropy(logits, targets)",
              "solution": "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))",
              "hints": [
                "CrossEntropy expects (N, C) logits and (N) targets. You must flatten Batch and Time."
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If vocab_size is 50257, what is the expected loss at initialization assuming uniform probability? (Round to 2 decimals)",
              "solution": "10.82",
              "hints": [
                "-ln(1/50257)"
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "The target for the token at index `t` is the token at index ___.",
              "template": "index ___.",
              "solution": "t+1"
            }
          ]
        },
        {
          "id": "gpt2_ch4_weight_tying_init",
          "title": "Weight Tying & Initialization",
          "video_id": "l8pRSuU81PU",
          "description": "Implement weight tying between the embedding layer and the language model head, and apply GPT-2 specific initialization rules (0.02 std, residual scaling).",
          "start": 3974,
          "end": 4938,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Linking `wte.weight` to `lm_head.weight`",
            "Applying standard deviation of 0.02 for initialization",
            "Scaling residual projections by 1/sqrt(2 * n_layers)"
          ],
          "tasks": [
            {
              "type": "code_ordering",
              "instruction": "Tie the weights of the language model head to the token embedding layer.",
              "lines": [
                "self.transformer.wte = nn.Embedding(vocab_size, n_embd)",
                "self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)",
                "self.lm_head.weight = self.transformer.wte.weight"
              ],
              "solution": [
                "self.transformer.wte = nn.Embedding(vocab_size, n_embd)",
                "self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)",
                "self.lm_head.weight = self.transformer.wte.weight"
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "For a model with 12 layers, what is the scaling factor for the residual projection initialization? (Round to 3 decimals)",
              "solution": "0.204",
              "hints": [
                "1 / sqrt(2 * 12)"
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why do we scale down the initialization of residual layers?",
              "options": [
                "To control the growth of variance in the residual stream",
                "To make the model train slower",
                "To prevent vanishing gradients in the attention mask",
                "Because GPT-2 uses Sigmoid activations"
              ],
              "solution": "To control the growth of variance in the residual stream"
            }
          ]
        },
        {
          "id": "gpt2_ch5_mixed_precision",
          "title": "Speed: Mixed Precision & TF32",
          "video_id": "l8pRSuU81PU",
          "description": "Accelerate training using TensorFloat-32 (TF32) and BFloat16 mixed precision, leveraging modern GPU tensor cores.",
          "start": 4938,
          "end": 6495,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Enabling TF32 on MatMul",
            "Using `torch.autocast` for BFloat16",
            "Understanding memory bandwidth vs compute bound"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "To enable TensorFloat-32, we set `torch.backends.cuda.matmul.___ = 'high'`.",
              "template": "allow___",
              "solution": "tf32"
            },
            {
              "type": "debug",
              "instruction": "The autocast context manager is missing the device type. Fix it.",
              "code": "with torch.autocast(dtype=torch.bfloat16):",
              "solution": "with torch.autocast(device_type='cuda', dtype=torch.bfloat16):",
              "hints": [
                "You must specify 'cuda' or 'cpu'."
              ]
            },
            {
              "type": "matching",
              "instruction": "Match the precision type to its mantissa size (precision).",
              "pairs": [
                {
                  "item": "FP32",
                  "match": "23 bits"
                },
                {
                  "item": "TF32",
                  "match": "10 bits (same as FP16)"
                },
                {
                  "item": "BF16",
                  "match": "7 bits"
                }
              ]
            }
          ]
        },
        {
          "id": "gpt2_ch6_kernels",
          "title": "Speed: torch.compile & Flash Attention",
          "video_id": "l8pRSuU81PU",
          "description": "Optimize GPU kernel execution using `torch.compile` and implement Flash Attention to reduce memory IO.",
          "start": 6495,
          "end": 7614,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Reducing Python overhead with `torch.compile`",
            "Kernel fusion mechanism",
            "Using `F.scaled_dot_product_attention` for Flash Attention"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "Why is Flash Attention faster than standard attention?",
              "options": [
                "It reduces memory IO (reads/writes) between HBM and SRAM",
                "It approximates the attention matrix",
                "It removes the Softmax operation",
                "It runs on the CPU"
              ],
              "solution": "It reduces memory IO (reads/writes) between HBM and SRAM"
            },
            {
              "type": "code_ordering",
              "instruction": "Replace the manual attention mask logic with the Flash Attention kernel.",
              "lines": [
                "# Old manual implementation removed",
                "y = F.scaled_dot_product_attention(q, k, v, is_causal=True)"
              ],
              "solution": [
                "# Old manual implementation removed",
                "y = F.scaled_dot_product_attention(q, k, v, is_causal=True)"
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "We pad the vocab size from 50257 to 50304. What is the smallest power of 2 that divides 50304?",
              "solution": "64",
              "hints": [
                "This aligns with GPU kernel block sizes."
              ]
            }
          ]
        },
        {
          "id": "gpt2_ch7_optimization",
          "title": "Hyperparameters & Schedulers",
          "video_id": "l8pRSuU81PU",
          "description": "Implement a Cosine Learning Rate Decay scheduler with Warmup, configure AdamW with specific parameters, and set up Gradient Clipping.",
          "start": 8095,
          "end": 9249,
          "xp_reward": 125,
          "key_learning_outcomes": [
            "Cosine Decay schedule mechanics",
            "Calculating Global Norm for Gradient Clipping",
            "Configuring Weight Decay groups (excluding biases)"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Implement the logic for the Learning Rate scheduler.",
              "lines": [
                "if step < warmup_steps:",
                "  return max_lr * (step + 1) / warmup_steps",
                "if step > max_steps:",
                "  return min_lr",
                "decay_ratio = (step - warmup) / (max - warmup)",
                "return min + 0.5 * (max - min) * (1 + cos(decay_ratio * pi))"
              ],
              "solution": [
                "if step < warmup_steps:",
                "  return max_lr * (step + 1) / warmup_steps",
                "if step > max_steps:",
                "  return min_lr",
                "decay_ratio = (step - warmup) / (max - warmup)",
                "return min + 0.5 * (max - min) * (1 + cos(decay_ratio * pi))"
              ]
            },
            {
              "type": "debug",
              "instruction": "Weight decay is being applied to LayerNorm weights. Fix the parameter grouping.",
              "code": "# param_dict contains all params\ndecay_params = [p for n, p in param_dict.items() if p.dim() >= 1]",
              "solution": "decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]",
              "hints": [
                "We usually only decay 2D matrices (embeddings, weights), not 1D vectors (biases, layernorm)."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Gradient clipping ensures the global norm of the gradient vector does not exceed ___.",
              "template": "exceed ___.",
              "solution": "1.0"
            }
          ]
        },
        {
          "id": "gpt2_ch8_scaling",
          "title": "Gradient Accumulation & DDP",
          "video_id": "l8pRSuU81PU",
          "description": "Scale training using Gradient Accumulation to simulate large batch sizes, and Distributed Data Parallel (DDP) for multi-GPU training.",
          "start": 9249,
          "end": 11421,
          "xp_reward": 150,
          "key_learning_outcomes": [
            "Simulating large batch sizes (0.5M tokens) via accumulation",
            "Normalizing loss by accumulation steps",
            "Using `dist.all_reduce` to average gradients across GPUs"
          ],
          "tasks": [
            {
              "type": "math_challenge",
              "instruction": "Target batch size = 524,288 tokens. Micro-batch = 16 sequences * 1024 tokens. Using 8 GPUs (DDP). How many gradient accumulation steps are needed?",
              "solution": "4",
              "hints": [
                "Micro_Batch_Size = 16 * 1024 = 16,384.",
                "Total_Immediate_Size = 16,384 * 8 GPUs = 131,072.",
                "Target / Total_Immediate = 524,288 / 131,072."
              ]
            },
            {
              "type": "debug",
              "instruction": "The loss is too high because gradients are not scaled during accumulation. Fix it.",
              "code": "loss = model(x, y)\nloss.backward()",
              "solution": "loss = model(x, y)\nloss = loss / grad_accum_steps\nloss.backward()",
              "hints": [
                "Since gradients sum up, the loss must be divided by the number of accumulation steps."
              ]
            },
            {
              "type": "quiz",
              "instruction": "In DDP, when should `optimizer.step()` be called?",
              "options": [
                "After the gradient accumulation loop finishes",
                "After every micro-batch forward pass",
                "Before the backward pass",
                "It happens automatically"
              ],
              "solution": "After the gradient accumulation loop finishes"
            }
          ]
        },
        {
          "id": "gpt2_ch9_results",
          "title": "FineWeb Data & Evaluation",
          "video_id": "l8pRSuU81PU",
          "description": "Switch to the high-quality FineWeb-Edu dataset, implement HellaSwag evaluation, and analyze the final training results matching GPT-3 performance.",
          "start": 11421,
          "end": 14419,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Sharding data for efficient loading",
            "Using HellaSwag for validation",
            "Comparing token efficiency vs GPT-2/3"
          ],
          "tasks": [
            {
              "type": "matching",
              "instruction": "Match the dataset to its description.",
              "pairs": [
                {
                  "item": "TinyShakespeare",
                  "match": "Character-level toy dataset"
                },
                {
                  "item": "FineWeb-Edu",
                  "match": "High-quality educational web crawl"
                },
                {
                  "item": "HellaSwag",
                  "match": "Sentence completion benchmark"
                }
              ]
            },
            {
              "type": "math_challenge",
              "instruction": "If the model processes 185,000 tokens/sec, roughly how long does it take to train on 10 Billion tokens? (in hours)",
              "solution": "15",
              "hints": [
                "10,000,000,000 / 185,000 / 3600"
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "In HellaSwag, the model picks the completion with the lowest average ___ per token.",
              "template": "lowest average ___.",
              "solution": "loss"
            }
          ]
        }
      ]
    }
   ]
}