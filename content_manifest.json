{
  "version": 1,
  "courses": [
    {
      "id": "course_intro_to_ai",
      "title": "Introduction to Artificial Intelligence",
      "thumbnail": "assets/ai-intro-thumbnail.jpg",
      "description": "A fundamental overview of AI, distinguishing between Weak and Strong AI, and exploring the hierarchy of Machine Learning and Deep Learning.",
      "sequence_order": 1,
      "chapters": [
        {
          "id": "intro_ai_ch1",
          "title": "Defining AI and Its Capabilities",
          "video_id": "ad79nYk2keg",
          "description": "We define Artificial Intelligence by looking at a robot's ability to adapt, reason, and solve problems. We then categorize AI into Weak (Narrow) and Strong forms, and clarify the relationship between AI, Machine Learning, and Deep Learning.",
          "start": 0,
          "end": 327,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Core Capabilities: Generalized Learning, Reasoning, Problem Solving",
            "Distinction between Weak AI (AlphaGo, Alexa) and Strong AI (Ultron)",
            "The hierarchy: AI > Machine Learning > Deep Learning",
            "Future concepts: The Singularity and Cyborgs"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Reconstruct the logic flow of the robot's capabilities as demonstrated in the field experiment.",
              "solution": "robot.adapt_to_lighting() # Generalized Learning\npath = robot.choose_path() # Reasoning\nrobot.use_plank_to_cross() # Problem Solving",
              "lines": [
                "robot.use_plank_to_cross() # Problem Solving",
                "robot.adapt_to_lighting() # Generalized Learning",
                "path = robot.choose_path() # Reasoning"
              ],
              "hints": [
                "First, the robot had to handle environmental changes.",
                "Second, it had to make a choice between roads.",
                "Finally, it had to overcome a physical obstacle."
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why is Alexa considered 'Weak AI' (Narrow AI) despite its many features?",
              "options": [
                "It operates based on pre-trained keywords and lacks self-awareness.",
                "It cannot connect to the internet.",
                "It controls physical robots."
              ],
              "solution": "It operates based on pre-trained keywords and lacks self-awareness.",
              "hints": [
                "Try asking it something it wasn't trained for, like traffic status without prior setup.",
                "Weak AI focuses on specific tasks."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Complete the definition of Deep Learning based on the video's description.",
              "template": "# Machine Learning uses algorithms to learn from data\n# Deep Learning is a subset inspired by the:\nmodel_inspiration = \"___ brain\"",
              "solution": "human",
              "hints": [
                "It mimics the biological organ in our heads.",
                "This approach helps better perceive patterns."
              ]
            },
            {
              "type": "quiz",
              "instruction": "What is the 'Point of Singularity' predicted by Ray Kurzweil?",
              "options": [
                "The year 2045, when robots become as smart as humans.",
                "The moment AI becomes dangerous.",
                "The point where we stop using smartphones."
              ],
              "solution": "The year 2045, when robots become as smart as humans.",
              "hints": [
                "It involves a specific year mentioned in the video.",
                "It relates to the parity between human and machine intelligence."
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "course_neural_networks_intro",
      "title": "Neural Networks: The Structure",
      "thumbnail": "assets/neural-networks-intro-thumbnail.jpg",
      "description": "A deep dive into the mathematical structure of neural networks, exploring how neurons, layers, weights, and biases collaborate to recognize handwritten digits.",
      "sequence_order": 2,
      "chapters": [
        {
          "id": "neural_nets_ch1",
          "title": "The Architecture of Layers",
          "video_id": "aircAruvnKk",
          "description": "Introduction to the biological inspiration behind neural networks, defining neurons as containers for activation numbers and explaining the hierarchical layered structure used to solve complex problems like digit recognition.",
          "start": 0,
          "end": 518,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Definition of a neuron's 'activation'",
            "Input and Output layer dimensions for MNIST",
            "The concept of Hidden Layers",
            "Hierarchical feature abstraction (Edges -> Loops -> Digits)"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "In this specific neural network model, what does a single 'neuron' essentially hold?",
              "options": [
                "A number between 0 and 1 representing activation",
                "A biological electrical impulse",
                "A complex image file"
              ],
              "solution": "A number between 0 and 1 representing activation",
              "hints": [
                "Think of it as a container for a single value.",
                "It represents how 'lit up' the cell is."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Calculate the input size. The input images are 28x28 pixels.",
              "template": "grid_width = 28\ngrid_height = 28\n# Each pixel is one neuron in the first layer\ntotal_input_neurons = ___",
              "solution": "784",
              "hints": [
                "Multiply the width by the height."
              ]
            },
            {
              "type": "parsons",
              "instruction": "Arrange the logic flow of how the network *should* hierarchically analyze an image of a number.",
              "solution": "Detect specific edges\nIdentify patterns (loops, lines)\nRecognize the full digit",
              "lines": [
                "Recognize the full digit",
                "Detect specific edges",
                "Identify patterns (loops, lines)"
              ],
              "hints": [
                "It starts with the smallest components.",
                "It ends with the final classification."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Complete the terminology for the layers between input and output.",
              "template": "input_layer = network[0]\noutput_layer = network[-1]\n# The layers in the middle are called:\nmiddle_layers = \"___ layers\"",
              "solution": "hidden",
              "hints": [
                "They are not directly seen as input or output.",
                "The video refers to them as '___ layers' because their function is initially a mystery."
              ]
            }
          ]
        },
        {
          "id": "neural_nets_ch2",
          "title": "Weights, Biases, and The Sigmoid",
          "video_id": "aircAruvnKk",
          "description": "Deconstructing the mechanism of a single neuron. This chapter covers how weights assign importance to input pixels, how biases act as activation thresholds, and how the sigmoid function normalizes the output.",
          "start": 518,
          "end": 806,
          "xp_reward": 75,
          "key_learning_outcomes": [
            "The role of Weights in pattern matching",
            "The Weighted Sum calculation",
            "The role of Bias in thresholding",
            "The Sigmoid 'squishification' function"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "If a neuron wants to detect a specific pixel pattern, how does it set its weights?",
              "options": [
                "Positive weights for matching pixels, negative for surrounding pixels",
                "All weights set to zero",
                "Random weights for all pixels"
              ],
              "solution": "Positive weights for matching pixels, negative for surrounding pixels",
              "hints": [
                "Think about the edge detection example.",
                "Green pixels meant positive, red meant negative."
              ]
            },
            {
              "type": "code",
              "instruction": "Implement the mathematical formula for a single neuron's processing *before* the activation function. Compute the weighted sum plus bias.",
              "template": "def calculate_z(inputs, weights, bias):\n    # inputs: list of pixel values\n    # weights: list of connection strengths\n    # bias: single number threshold\n    weighted_sum = 0\n    for i in range(len(inputs)):\n        weighted_sum += ___\n    return weighted_sum + ___",
              "solution": "def calculate_z(inputs, weights, bias):\n    # inputs: list of pixel values\n    # weights: list of connection strengths\n    # bias: single number threshold\n    weighted_sum = 0\n    for i in range(len(inputs)):\n        weighted_sum += inputs[i] * weights[i]\n    return weighted_sum + bias",
              "hints": [
                "Multiply the input by its corresponding weight.",
                "Don't forget to add the bias at the end."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Define the purpose of the Bias.",
              "template": "# The bias indicates how high the weighted sum needs to be\n# before the neuron becomes meaningfully ___",
              "solution": "active",
              "hints": [
                "It's about whether the neuron lights up or not."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "The output of the neuron is compressed into a 0-1 range using this function.",
              "template": "import math\n\ndef activation_function(x):\n    # This is the ___ function\n    return 1 / (1 + math.exp(-x))",
              "solution": "sigmoid",
              "hints": [
                "Also known as the logistic curve.",
                "Starts with 's'."
              ]
            }
          ]
        },
        {
          "id": "neural_nets_ch3",
          "title": "Matrix Notation & Modern Activations",
          "video_id": "aircAruvnKk",
          "description": "Transitioning to Linear Algebra notation for efficiency. This chapter explains how to represent layers as vectors and weights as matrices, and concludes with a discussion on ReLU vs Sigmoid.",
          "start": 806,
          "end": 1119,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Vector representation of activations",
            "Matrix-Vector multiplication for layers",
            "Compact mathematical notation sigma(Wa + b)",
            "ReLU (Rectified Linear Unit) definition"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Construct the clean linear algebra expression for the activation of the next layer.",
              "solution": "weighted_sum = np.dot(weights_matrix, activation_vector)\nz = weighted_sum + bias_vector\nnext_activations = sigmoid(z)",
              "lines": [
                "next_activations = sigmoid(z)",
                "weighted_sum = np.dot(weights_matrix, activation_vector)",
                "z = weighted_sum + bias_vector"
              ],
              "hints": [
                "Matrix multiplication happens first.",
                "Add the bias vector second.",
                "Apply the function last."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "In the matrix notation, what does 'W' represent?",
              "template": "# a(L) = sigmoid( W * a(L-1) + b )\n# W is the matrix containing all ___ connecting the two layers",
              "solution": "weights",
              "hints": [
                "These are the connection strengths."
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why do modern networks often use ReLU instead of Sigmoid?",
              "options": [
                "ReLU is easier to train for deep networks",
                "ReLU outputs numbers between 0 and 1 only",
                "ReLU mimics the biology perfectly"
              ],
              "solution": "ReLU is easier to train for deep networks",
              "hints": [
                "Lisha Li mentions this in the interview.",
                "Sigmoid was 'old school'."
              ]
            },
            {
              "type": "code",
              "instruction": "Implement the ReLU function described in the interview.",
              "template": "def relu(x):\n    # Return x if x is positive, else 0\n    return ___",
              "solution": "def relu(x):\n    # Return x if x is positive, else 0\n    return max(0, x)",
              "hints": [
                "Use the max function.",
                "Compare x against 0."
              ]
            }
          ]
        }
      ]
    }, 
    {
      "id": "course_micrograd",
      "title": "Building Micrograd: Backpropagation from Scratch",
      "thumbnail": "assets/micrograd-thumbnail.jpg",
      "description": "Master the fundamentals of neural networks by building 'micrograd', a tiny Autograd engine, from scratch in Python.",
      "sequence_order": 3,
      "chapters": [
        {
          "id": "micrograd_ch1",
          "title": "The Intuition of Derivatives",
          "video_id": "VMj-3S1tku0",
          "description": "We start by defining the derivative conceptually as the 'rise over run' sensitivity of a function to its inputs. We implement a simple numerical estimation of the derivative for scalar functions. ",
          "start": 0,
          "end": 1149,
          "xp_reward": 30,
          "key_learning_outcomes": [
            "Definition of Derivative (Limit as h->0)",
            "Numerical estimation of slope",
            "Understanding function sensitivity"
          ],
          "tasks": [
            {
              "type": "code",
              "instruction": "Implement the formula for the numerical approximation of a derivative.",
              "template": "def get_derivative(f, x, h=0.0001):\n    # Calculate rise over run\n    return (f(x + h) - ___) / ___",
              "solution": "def get_derivative(f, x, h=0.0001):\n    # Calculate rise over run\n    return (f(x + h) - f(x)) / h",
              "hints": [
                "Subtract the function value at x from the value at x+h.",
                "Divide by the step size h."
              ]
            },
            {
              "type": "quiz",
              "instruction": "If a function f(x) has a derivative of -3.0 at x=2.0, what does this imply?",
              "options": [
                "Increasing x slightly will decrease the output.",
                "Increasing x slightly will increase the output.",
                "The function is flat at x=2.0."
              ],
              "solution": "Increasing x slightly will decrease the output.",
              "hints": [
                "A negative slope means the function goes down as you go right."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch2",
          "title": "The Value Object & Visualization",
          "video_id": "VMj-3S1tku0",
          "description": "We define the core `Value` class to wrap scalar numbers, allowing us to track the history of operations for the computation graph. We implement `__add__` and `__mul__` to overload Python operators.",
          "start": 1149,
          "end": 1930,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Creating a Wrapper Class for scalars",
            "Operator Overloading (__add__, __mul__)",
            "Building the computation graph structure (children, ops)"
          ],
          "tasks": [
            {
              "type": "gap_fill",
              "instruction": "Complete the Value class initialization to track graph connectivity.",
              "template": "class Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        # Store the previous nodes to build the graph\n        self._prev = set(___)\n        self._op = _op",
              "solution": "_children",
              "hints": [
                "We need to convert the tuple of children into a set for the previous nodes."
              ]
            },
            {
              "type": "code",
              "instruction": "Implement the addition operator overloading.",
              "template": "def __add__(self, other):\n    # Create a new Value that is the sum of self and other\n    out = Value(self.data + other.data, (self, other), '+')\n    return out",
              "solution": "def __add__(self, other):\n    out = Value(self.data + other.data, (self, other), '+')\n    return out"
            }
          ]
        },
        {
          "id": "micrograd_ch3",
          "title": "Manual Backpropagation: The Basics",
          "video_id": "VMj-3S1tku0",
          "description": "We begin manually calculating derivatives for a simple expression graph. We discover how addition nodes 'route' gradients and multiplication nodes 'switch' gradients based on input values.",
          "start": 1930,
          "end": 2634,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Local Derivatives for Addition and Multiplication",
            "Routing vs Switching Gradients",
            "Understanding `grad` accumulation"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "In the expression `d = a + b`, if `dL/dd = 2.0`, what is `dL/da`?",
              "options": [
                "2.0 (The addition node distributes the gradient equally)",
                "1.0 (It divides the gradient)",
                "0.0"
              ],
              "solution": "2.0 (The addition node distributes the gradient equally)",
              "hints": [
                "The local derivative of addition is 1.0.",
                "Multiply global gradient by local derivative."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "For `d = a * b`, the gradient with respect to `a` depends on `b`.",
              "template": "# d = a * b\n# grad_a = grad_d * local_derivative\n# local_derivative of (a*b) with respect to a is ___\ngrad_a = grad_d * b.data",
              "solution": "b.data",
              "hints": [
                "Differentiation of ax with respect to x is a."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch4",
          "title": "The Chain Rule & Optimization",
          "video_id": "VMj-3S1tku0",
          "description": "We formalize the backpropagation logic using the Chain Rule from calculus, multiplying local derivatives by global gradients. We then perform a single manual optimization step to reduce the output.",
          "start": 2634,
          "end": 3172,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "The Chain Rule Formula",
            "Recursive application of gradients",
            "Nudging inputs to minimize/maximize output"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the logic for the Chain Rule as explained in the video.",
              "solution": "Find the derivative of the output w.r.t the current node\nMultiply it by the local derivative of the current node w.r.t input\nResult is the derivative of the output w.r.t the input",
              "lines": [
                "Multiply it by the local derivative of the current node w.r.t input",
                "Find the derivative of the output w.r.t the current node",
                "Result is the derivative of the output w.r.t the input"
              ],
              "hints": [
                "Start with the gradient coming from 'above' (the output)."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "To minimize the output `L`, we should adjust input `a` in the direction ___ to the gradient.",
              "template": "# Gradient Descent Step\n# a.data += step_size * (___ * a.grad)",
              "solution": "-1.0",
              "hints": [
                "The gradient points in the direction of increase.",
                "We want to go the opposite way."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch5",
          "title": "Backpropagating a Neuron",
          "video_id": "VMj-3S1tku0",
          "description": "We build a single Neuron model and manually backpropagate gradients through the hyperbolic tangent activation function.",
          "start": 3172,
          "end": 4142,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Structure of a Neuron (Weights, Bias, Activation)",
            "Derivative of tanh (1 - t^2)",
            "Backpropagating through composite functions"
          ],
          "tasks": [
            {
              "type": "code",
              "instruction": "Implement the derivative calculation for the Tanh activation function, assuming `t` is the Tanh output.",
              "template": "def tanh_backward(t, grad_output):\n    # d/dx tanh(x) = 1 - tanh(x)^2\n    local_derivative = 1 - (t ** ___)\n    return grad_output * local_derivative",
              "solution": "2",
              "hints": [
                "It is one minus the square of the output."
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why was the gradient for `w2` zero in the example shown?",
              "options": [
                "Because the input `x2` was zero, killing the gradient flow during multiplication.",
                "Because the weight was initialized to zero.",
                "Because tanh squashed it to zero."
              ],
              "solution": "Because the input `x2` was zero, killing the gradient flow during multiplication.",
              "hints": [
                "In multiplication d(w*x)/dw = x. If x is 0, what is the derivative?"
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch6",
          "title": "Automating Gradients",
          "video_id": "VMj-3S1tku0",
          "description": "We remove the manual work by implementing `_backward` closures for each operation and using Topological Sort to call them in the correct order automatically.",
          "start": 4142,
          "end": 4948,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Closures for backward passes",
            "Topological Sort for graph traversal",
            "The `backward()` method"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Arrange the steps for the automatic backward pass function.",
              "solution": "topo = build_topological_sort(self)\nself.grad = 1.0\nfor node in reversed(topo):\n    node._backward()",
              "lines": [
                "self.grad = 1.0",
                "for node in reversed(topo):\n    node._backward()",
                "topo = build_topological_sort(self)"
              ],
              "hints": [
                "We need the graph order first.",
                "We must initialize the gradient of the root node before propagating.",
                "We traverse in reverse topological order."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Define the backward closure for addition inside `__add__`.",
              "template": "def _backward():\n    self.grad += 1.0 * out.grad\n    other.grad += ___ * out.grad",
              "solution": "1.0",
              "hints": [
                "Addition distributes the gradient equally to both parents."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch7",
          "title": "Refining the Engine: Bugs & Powers",
          "video_id": "VMj-3S1tku0",
          "description": "We identify and fix a critical bug where gradients were being overwritten instead of accumulated. We then implement subtraction, division, and power operations to handle complex expressions.",
          "start": 4948,
          "end": 5971,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "The Gradient Accumulation Bug (+= vs =)",
            "Implementing Power Rule (x^k)",
            "Implementing Division as x * y^-1"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "What happens if a variable is used multiple times in a graph (e.g., `b = a + a`) and we use `=` instead of `+=` for gradients?",
              "options": [
                "The gradient is incorrect because it only keeps the last branch's contribution.",
                "The gradient causes an infinite loop.",
                "Nothing, it works fine."
              ],
              "solution": "The gradient is incorrect because it only keeps the last branch's contribution.",
              "hints": [
                "Gradients from different paths must sum up (multivariate chain rule)."
              ]
            },
            {
              "type": "code",
              "instruction": "Implement the backward pass for the Power operation (x^k).",
              "template": "def _backward():\n    # d/dx (x^k) = k * x^(k-1)\n    self.grad += (other * self.data ** (other - 1)) * ___",
              "solution": "out.grad",
              "hints": [
                "Don't forget to multiply by the incoming global gradient from the output."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch8",
          "title": "PyTorch & Neural Network Modules",
          "video_id": "VMj-3S1tku0",
          "description": "We compare our engine to PyTorch's API and build higher-level classes: `Neuron`, `Layer`, and `MLP` (Multi-Layer Perceptron) to mimic deep learning libraries.",
          "start": 5971,
          "end": 6664,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "PyTorch Tensor API alignment",
            "Defining a Neuron class",
            "Defining an MLP (Multi-Layer Perceptron) class"
          ],
          "tasks": [
            {
              "type": "code",
              "instruction": "Implement the forward pass for a single Neuron.",
              "template": "def __call__(self, x):\n    # w * x + b\n    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n    out = act.tanh()\n    return ___",
              "solution": "out",
              "hints": [
                "Simply return the activated output."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "An MLP is a sequence of Layers.",
              "template": "class MLP(Module):\n    def __init__(self, nin, nouts):\n        sz = [nin] + nouts\n        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(___)\n        return x",
              "solution": "x",
              "hints": [
                "We pass the output of one layer as input to the next."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch9",
          "title": "Loss & Parameters",
          "video_id": "VMj-3S1tku0",
          "description": "We define a Mean Squared Error (MSE) loss function to measure performance and implement a method to recursively collect all parameters (weights and biases) from the network.",
          "start": 6664,
          "end": 7272,
          "xp_reward": 50,
          "key_learning_outcomes": [
            "Mean Squared Error (MSE) Loss formula",
            "Recursively collecting parameters",
            "Purpose of Loss functions"
          ],
          "tasks": [
            {
              "type": "code",
              "instruction": "Calculate the MSE loss for a single prediction and target.",
              "template": "diff = prediction - target\nloss = diff ** ___\n# In a real batch, we would average this",
              "solution": "2",
              "hints": [
                "MSE stands for Mean SQUARED Error."
              ]
            },
            {
              "type": "gap_fill",
              "instruction": "Collect parameters from all neurons in a layer.",
              "template": "class Layer(Module):\n    def parameters(self):\n        return [p for neuron in self.neurons for p in neuron.___()]",
              "solution": "parameters",
              "hints": [
                "Call the parameters method of the individual neuron."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch10",
          "title": "The Training Loop",
          "video_id": "VMj-3S1tku0",
          "description": "We implement the full Gradient Descent loop: Forward pass, Zero Gradients, Backward pass, and Update. We debug a common issue where gradients accumulate incorrectly across steps.",
          "start": 7272,
          "end": 8043,
          "xp_reward": 100,
          "key_learning_outcomes": [
            "Gradient Descent Steps (Forward, Backward, Update)",
            "The necessity of `zero_grad`",
            "Learning Rate tuning"
          ],
          "tasks": [
            {
              "type": "parsons",
              "instruction": "Order the steps of a single training iteration.",
              "solution": "loss = model(inputs) # Forward pass\nmodel.zero_grad()\nloss.backward()\nfor p in model.parameters():\n    p.data += -0.01 * p.grad",
              "lines": [
                "loss.backward()",
                "model.zero_grad()",
                "loss = model(inputs) # Forward pass",
                "for p in model.parameters():\n    p.data += -0.01 * p.grad"
              ],
              "hints": [
                "Calculate loss first.",
                "Clear old gradients before calculating new ones.",
                "Backpropagate.",
                "Update weights."
              ]
            },
            {
              "type": "quiz",
              "instruction": "Why did the network train too fast and then become unstable before adding `zero_grad()`?",
              "options": [
                "The gradients were accumulating every step, effectively increasing the step size uncontrollably.",
                "The learning rate was set to 100.0.",
                "The random initialization was lucky."
              ],
              "solution": "The gradients were accumulating every step, effectively increasing the step size uncontrollably.",
              "hints": [
                "Remember the `+=` bug fix? It applies across training steps too if not cleared."
              ]
            }
          ]
        },
        {
          "id": "micrograd_ch11",
          "title": "Conclusion & Real-World Application",
          "video_id": "VMj-3S1tku0",
          "description": "We wrap up by reviewing the full code structure, looking at a more complex binary classification demo, and diving into the actual PyTorch codebase to find the `tanh` implementation.",
          "start": 8043,
          "end": 8751,
          "xp_reward": 30,
          "key_learning_outcomes": [
            "Micrograd vs Production Libraries (PyTorch)",
            "Understanding Stride/Offset in Tensors (Briefly)",
            "Real-world complexity of autograd kernels"
          ],
          "tasks": [
            {
              "type": "quiz",
              "instruction": "When looking at the real PyTorch code, why was the `tanh` implementation so hard to find?",
              "options": [
                "It is buried in complex dispatch files involving CPU/GPU kernels and optimizations.",
                "It doesn't exist; PyTorch uses magic.",
                "It was in the root folder."
              ],
              "solution": "It is buried in complex dispatch files involving CPU/GPU kernels and optimizations.",
              "hints": [
                "Production libraries prioritize efficiency and hardware support over readability."
              ]
            }
          ]
        }
      ]
    }

  ]
}